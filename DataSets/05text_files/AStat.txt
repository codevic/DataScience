global estimation problem drift function considered large class ergodic diffusion processes unknown drift supposed belong nonparametric class smooth functions order k value k known statistician fully data driven procedure estimating drift function proposed estimated risk minimization method sharp adaptivity procedure optimal constant quality estimation measured integrated squared error weighted square invariant density investigates conditional quasi likelihood ratio test threshold ma models threshold test statistic converges weakly function centred gaussian process local alternatives test nontrivial asymptotic power based new weak convergence linear marked empirical process independently interest gives invertible expansion threshold ma models binomial type operator stationary gaussian process introduced order model long memory spatial context consistent estimators model parameters demonstrated particular d cap n d op log n n d d d denotes long memory parameter proposes class goodness fit tests autocorrelation function time series process including exhibiting long range dependence test composite hypotheses functionals approximated martingale transformation bartlett p process estimated parameters converges distribution standard brownian motion null discuss tests different natures omnibus directional portmanteau type tests monte carlo illustrates performance different tests practice focuses recursive estimation time varying autoregressive processes nonparametric setting stability model revisited uniform provided time varying autoregressive parameters belong appropriate smoothness classes adequate normalization correction term used recursive estimation procedure allows mild assumptions innovations distributions rate convergence pointwise estimates minimax beta lipschitz classes beta beta property longer holds seen asymptotic expansion estimation error bias reduction method proposed recovering minimax rate solve problem constructing asymptotic global confidence region means covariance matrices reproduction distributions involved supercritical multitype branching process approach based central limit theorem associated quadratic law large numbers performed maximum likelihood multidimensional lotka nagaev estimator reproduction law means extension approach least squares estimator mean matrix briefly discussed mixed linear models nonnormal data gaussian fisher information matrix called quasi information matrix quim quim plays role evaluating asymptotic covariance matrix estimators model parameters including variance components traditionally two ways estimate information matrix estimated information matrix observed one analytic form quim involves parameters variance components example third fourth moments random effects estimated quim available hand dependence normormality data observed quim inconsistent propose estimator quim consists partially observed form partially estimated one estimator consistent computationally easy operate method used derive large sample tests statistical hypotheses involve variance components non gaussian mixed linear model finite sample performance test studied simulations compared delete group jackknife method applies special case non gaussian mixed linear models general structural equation model fitted panel data set consists correlated samples correlated samples could data correlated populations correlated observations occasions panel data consider cases full pseudo normal likelihood cannot used example highly unbalanced data participating individuals appear consecutive model estimated partial likelihood would full correct likelihood independent normal samples proved asymptotic standard errors e parameters overall fit measure corresponding ones derived standard assumptions normality independence observations since allow us apply classical statistical methods inference first second order moments correlated nonnormal data via simulation e based first two moments negligible bias less variability e computed alternative robust estimator utilizes fourth moments methodology applied real panel data correlated samples cannot formulated analyzed independent samples robust e remaining parameters additionally simulation efficiency loss considering correlation samples small negligible cases random fixed variables determine optimal designs regression models frequently used describing three dimensional shapes models based fourier expansion function defined unit sphere terms spherical harmonic basis functions particular demonstrated uniform distribution sphere optimal respect phi p criteria proposed kiefer optimal respect criterion maximizes p mean r smallest eigenvalues variance covariance matrix criterion related principal component analysis common tool analyzing type image data moreover discrete designs sphere derived yield information matrix spherical harmonic regression model uniform distribution directly implementable practice demonstrated new designs substantially efficient commonly used designs three dimensional shape analysis establish mathematical framework formally validates twophase super population viewpoint proposed hartley sielken biometrics defining product probability space includes design space model space methodology develop combines finite population sampling theory classical theory infinite population sampling account underlying processes produce data unified approach key following first sample estimators converge design law model converge model certain conditions asymptotically independent converge jointly product space second sample estimating equation estimator asymptotically normal around super population parameter supersaturated design design whose run size large enough estimating main effects goodness multi level supersaturated designs judged generalized minimum aberration criterion proposed xu wu ann statist new bound derived general construction methods proposed multi level supersaturated designs inspired addelman kempthorne construction orthogonal arrays several classes optimal multi level supersaturated designs given explicit form columns labeled linear quadratic polynomials rows points finite field additive characters used properties resulting designs small optimal supersaturated designs levels listed properties aims generalize unity classical criteria comparisons balanced lattice designs including fractional factorial designs supersaturated designs uniform designs present general majorization framework assessing designs includes stringent criterion majorization via pairwise coincidences flexible surrogates via convex functions classical orthogonality aberration uniformity criteria unified choosing combinatorial exponential kernels construction method sketched balanced crossover designs given patterson biometrika universally optimal uo joint estimation direct residual effects competing class class connected binary designs b uo estimation direct residual effects competing class designs class connected designs includes connected binary designs treatment given consecutive periods formulation uo given shah sinha unpublished manuscript introduce functional practical interest involving direct residual effects establish c optimality patterson designs respect functional class competing designs b propose nonparametric methods functional linear regression designed sparse longitudinal data predictor response functions covariate time predictor response processes smooth random trajectories data consist small number noisy repeated measurements made irregular times sample longitudinal number repeated measurements per often small may modeled discrete random number accordingly finite asymptotically nonincreasing number measurements available experimental unit propose functional regression approach situation functional principal component analysis estimate functional principal component scores conditional expectations allows prediction unobserved response trajectory sparse measurements predictor trajectory resulting technique flexible allows different patterns regarding timing measurements obtained predictor response trajectories asymptotic properties sample n investigated mild conditions n infinity obtain consistent estimation regression function besides convergence components functional linear regression regression parameter function construct asymptotic pointwise confidence bands predicted trajectories functional coefficient determination measure variance explained functional regression model introduced extending standard r functional case proposed methods illustrated simulation longitudinal primary biliary liver cirrhosis data analysis longitudinal relationship blood pressure body mass index suggest two nonparametric approaches based kernel methods orthogonal series estimating regression functions presence instrumental variables first time class problems derive optimal convergence rates attained particular estimators presence instrumental variables relation identifies regression function defines ill posed inverse problem difficulty depends eigenvalues certain integral operator determined joint density endogenous instrumental variables delineate role played problem difficulty determining optimal convergence rate appropriate choice smoothing parameter estimation quadratic functional parameter spaces quadratically convex considered contrast theory quadratically convex parameter spaces optimal quadratic rules often rate suboptimal cases minimax rate optimal procedures constructed based local thresholding nonquadratic procedures sometimes fully efficient even optimal quadratic rules slow rates convergence moreover estimating quadratic functional nonquadratic procedures may exhibit different elbow phenomena quadratic procedures classes coordinate invariant omnibus goodness fit tests compact riemannian manifolds proposed tests based gine sobolev tests uniformity condition consistency given tests illustrated example rotation group bayesian methods developed multivariate nonparametric regression problem domain taken compact riemannian manifold terms latter underlying geometry manifold induces certain symmetries multivariate nonparametric regression function bayesian approach allows one incorporate hierarchical bayesian methods directly spectral structure thus providing symmetry adaptive multivariate bayesian function estimator one diffuse away prior information limiting case smoothing spline manifold together result smoothing spline solution obtains minimax rate convergence multivariate nonparametric regression problem good frequentist properties bayes estimators application astronomy included recursive monte carlo filters called particle filters powerful tool perform computations general state space models discuss compare accept reject version common sampling importance resampling version algorithm particular auxiliary variable methods stratification used accept reject version compare different resampling techniques second part laws large numbers central limit theorem monte carlo filters simple induction arguments need weak conditions stronger conditions required sample size independent length observed series concerns estimation sums functions observable unobservable variables bounds asymptotic variance convolution theorem derived general finite infinite dimensional models explicit relationship established efficient influence functions estimation sums variables estimation means certain plug estimators proved asymptotically efficient finite dimensional models u v estimators robbins proved efficient infinite dimensional mixture models examples include certain species network data confidentiality problems multivariate normal mixtures flexible method fitting dimensional data topography sense key features density analyzed rigorously dimensions ridgeline manifold contains critical points well ridges density plot elevations ridgeline key features mixed density addition ridgeline uncover function determines number modes mixed density two components mixed followup analysis gives curvature function used set modality theorems mixture density meant density form pi mu f pi theta x mu d theta pi theta theta theta element family probability densities mu probability measure theta consider problem identifying unknown part model mixing distribution finite sample independent observations pi mu assuming mixing distribution density function wish estimate density within appropriate function classes general approach proposed scope application investigated case discrete distributions mixtures power series distributions specifically studied standard methods density estimation kernel estimators available context methods rate optimal almost rate optimal balls various smoothness spaces instance apply mixtures poisson distribution parameterized mean estimators based oil orthogonal polynomial sequences proposed achieve similar rates general approach extends simplifies instance allows lis asymptotic minimax efficiency certain smoothness classes mentioned polynomial estimator poisson case discrete location mixtures discrete deconvolution mixtures discrete uniform distributions right censored survival data collected cohort prevalent cases constant incidence length biased may used estimate length biased e prevalent case survival function incidence rate constant called stationarity incidence efficient structure unconditional statistical inference carry analysis conditioning observed truncation times well known due informative censoring prevalent cohort data kaplan meier estimator unconditional npmle length biased survival function asymptotic properties npmle follow known result present detailed derivation asymptotic properties npmle length biased survival function right censored prevalent cohort survival data follow particular npmle uniformly strongly consistent converges weakly gaussian process asymptotically efficient one spin yield asymptotic properties npmle incident case survival function see asgharian m lan wolfson j amer statist assoc often prime interest prevalent cohort generalize given vardi zhang ann statist multiplicative censoring arises degenerate case prevalent cohort setting maximum likelihood estimation extensively used joint analysis repeated measurements survival time lack theoretical justification asymptotic properties maximum likelihood estimators intends fill gap specifically consistency maximum likelihood estimators derive asymptotic distributions maximum likelihood estimators semi parametrically efficient signals belonging balls smoothness classes noise enough moments asymptotic behavior minimax quadratic risk among soft threshold estimates investigated turn combined median filtering method lead asymptotics denoising heavy tails via wavelet thresholding comparisons wavelet thresholding kernel estimators briefly discussed estimation density regression errors fundamental issue regression analysis typically explored via parametric approach uses nonparametric approach mean integrated squared error mise criterion solves long standing problem formulated two decades ago mark pinsker estimation nonparametric error density nonparametric regression setting accuracy oracle knows underlying regression errors solution implies mild assumption differentiability design density regression function mise data driven error density estimator attains minimax rates sharp constants known case directly observed regression errors result holds error densities finite infinite supports extensions result general heteroscedastic models possibly dependent errors predictors obtained latter case marginal error density estimated considered cases blockwise shrinking efromovich pinsker density estimate based plugged residuals used obtained imply theoretical justification customary practice applied regression analysis consider residuals proxies underlying regression errors numerical real examples presented discussed plus software available investigate limit behavior l k distance decreasing density f nonparametric maximum likelihood estimator f k due inconsistency f cap n zero case k turns kind transition point extend asymptotic normality l distance lk distance k obtain analogous limiting result modification l k distance k since l distance area f f cap n area inverse g f tractable inverse u f problem reduced immediately deriving asymptotic normality l distance u n g although lose easy correspondence k l k distance f f cap n asymptotically equivalent l k distance u n g consider partly linear transformation models applied current status data unknown quantities transformation function linear regression parameter nonparametric regression effect penalized mle regression parameter asymptotically normal efficient converges parametric rate although penalized mle transformation function nonparametric regression effect n consistent inference regression parameter based block jackknife investigated computational issues proposed methodology simulation transformation models partly linear regression terms coupled new estimation inference techniques flexible alternatives cox model current status data analysis popular data driven method choosing bandwidth standard kernel regression cross validation even outliers ill data robust kernel regression used estimate unknown regression curve robust nonlinear time series analysis lecture notes statist circumstances standard cross validation longer satisfactory bandwidth selector unduly influenced extreme prediction errors caused existence outliers robust method proposed cross validation method discounts extreme prediction errors large samples robust method chooses consistent bandwidths consistency method practically independent form ill extreme prediction errors discounted additionally evaluation method finite sample behavior simulation proposed method performs favorably method call applied problems example model selection require cross validation adaptive estimation linear functionals collection parameter spaces considered class modulus continuity geometric quantity instrumental characterizing degree adaptability two parameter spaces way usual modulus continuity captures minimax difficulty estimation single parameter space general construction optimally adaptive estimators based ordered modulus continuity given complemented several illustrative examples stein statist sci proposed matern type gaussian random fields flexible class models computer experiments considers subclass models exactly mean square differentiable particular likelihood function determined closed form mild conditions sieve maximum likelihood estimators parameters covariance function weakly consistent respect fixed domain asymptotics construct moment partial sum processes based residuals garch model mean known consider partial sums kill powers residuals cusum processes self normalized partial sum processes kth power partial sum process converges brownian process plus correction term correction term depends kill moment mu k innovation sequence mu k correction term thus kth power partial sum process converges weakly gaussian process kill power partial sum d innovations sequence particular since mu holds first moment partial sum process fails second moment partial sum process consider cusum self normalized processes standardized residual sample variance behave residuals asymptotically d joint distribution kth k st self normalized partial sum processes applications change point problems goodness fit considered particular cusum testing garch model structure change jarque bera omnibus statistic testing normality unobservable innovation distribution garch model residuals constructing kernel density function estimation innovation distribution discussed assume observations generated ail infinite order autoregressive ar infinity process shibata ann statist considered problem choosing finite order ar model allowing order become infinite number observations order obtain better approximation showed purpose predicting future ail independent replicate akaike information criterion aic variants asymptotically efficient although shibata concept asymptotic efficiency widely accepted natural property time series analysis new observations time series become available independent previous data overcome difficulty focus oil order selection forecasting future observed time series referred realization prediction present first theoretical verification aic variants still asymptotically efficient sense defined ill section realization predictions obtain result technical condition easily met common practice introduced simplify complicated dependent structures among selected orders estimated parameters future observations addition simulation conducted illustrate practical implications aic aic yields satisfactory realization prediction finite samples oil hand limitation aic realization settings pointed interesting note limitation aic exist corresponding independent cases efron j roy statist soc ser b proposed computationally efficient method called jackknife bootstrap estimating variance bootstrap estimator independent data dependent data version jackk iiife bootstrap method recently proposed lahiri econometric theory jackknife bootstrap estimators variance bootstrap quantile consistent dependent independent data simulation presented introduce several measures complexity functions convex hull given base class complexity measures take account sparsity weights convex combination well certain clustering properties base functions involved new upper confidence bounds generalization error ensemble voting classification algorithms utilize new complexity measures along empirical distributions classification margins providing better explanation generalization performance large margin classification methods propose new bounds error learning algorithms terms data dependent notion complexity estimates establish give optimal rates based local empirical version rademacher averages sense rademacher averages computed data subset functions small empirical error present applications classification prediction convex function classes kernel classes particular boosting one significant machine learning classification regression original computationally flexible version boosting seeks minimize empirically loss function greedy fashion resulting estimator takes additive function form built iteratively applying base estimator learner updated samples depending previous iterations unusual regularization technique early stopping employed based cv test set numerical convergence consistency statistical rates convergence boosting early stopping carried linear span family basis functions general loss functions convergence boosting greedy optimization infinimum loss function linear span numerical convergence result early stopping strategies boosting consistent based d samples obtain bounds rates convergence boosting estimators simulation presented illustrate relevance theoretical providing insights practical aspects boosting side product reveal importance restricting greedy search step sizes known practice work friedman others moreover lead rigorous proof linearly separable problem adaboost epsilon step size becomes l margin maximizer left run convergence propose novel approach sufficient dimension reduction regression based estimating contour directions small variation response directions span orthogonal complement minimal space relevant regression extracted according two measures variation response leading simple general contour regression scr gcr methodology comparison existing sufficient dimension reduction techniques contour based methodology guarantees exhaustive estimation central subspace ellipticity predictor distribution mild additional assumptions maintaining root n consistency computational ease moreover proves robust departures ellipticity establish population properties scr gcr asymptotic properties scr simulations compare performance standard techniques ordinary least squares sliced inverse regression principal hessian directions sliced average variance estimation confirm advantages anticipated theoretical analyses contour based methods data set concerning soil evaporation variable selection fundamental dimensional statistical modeling many variable selection techniques may implemented maximum penalized likelihood various penalty functions optimizing penalized likelihood function often challenging may nondifferentiable nonconcave proposes new class algorithms finding maximizer penalized likelihood broad class penalty functions algorithms operate perturbing penalty function slightly render differentiable optimizing differentiable function minorize maximize mm algorithm mm algorithms useful extensions well known class em algorithms fact allows us analyze local global convergence proposed algorithm techniques employed em algorithms particular mm algorithms converge must converge desirable point discuss conditions convergence may guaranteed exploit newton raphson like aspect algorithms propose sandwich estimator standard errors estimators method performs well numerical tests consider statistical analysis data dimensional spheres shape spaces work particular relevance applications dimensional data available commonly encountered situation many disciplines first uniform measure infinite dimensional sphere reviewed together connections wiener measure discuss densities gaussian measures respect wiener measure nonuniform distributions infinite dimensional spheres shape spaces introduced special cases practical consequences considered focus dimensional real complex bingham uniform von mises fisher fisher bingham real complex watson distributions asymptotic distributions cases dimension sample size large discussed approximations practical maximum likelihood based inference considered particular discuss application brain shape modeling deals projective shape analysis finite configurations points modulo projective transformations topic various applications machine vision introduce convenient projective shape space well appropriate coordinate system shape space generic configurations k points m dimensions resulting projective shape space identified product k m copies axial spaces rpm identification leads need developing multivariate directional multivariate axial analysis propose parametric models well nonparametric methods areas particular investigate frechet extrinsic mean multivariate axial case asymptotic distributions appropriate parametric nonparametric tests derived illustrate methodology examples machine vision explores class empirical bayes methods level dependent threshold selection wavelet shrinkage prior considered wavelet coefficient mixture atom probability zero heavy tailed density mixing weight sparsity parameter level transform chosen marginal maximum likelihood estimation carried posterior median random thresholding procedure estimation carried thresholding rules threshold details calculations needed implementing procedure included practice estimates quick compute software available simulations standard model functions excellent performance applications data drawn various fields application used explore practical performance approach general result risk corresponding marginal maximum likelihood approach single sequence overall bounds risk method found membership unknown function one wide range besov classes covering case f bounded variation rates obtained optimal value parameter p infinity simultaneously wide range loss functions dominating l q norm sigma th derivative q attention paid distinction sampling unknown function within white noise sampling discrete points placing constraints function discrete wavelet transform sequence values observation points relevant combinations scenarios obtained cases key feature theory particular boundary corrected wavelet basis details discussed overall approach described seems far unique combining properties fast computation good theoretical properties good performance simulations practice key feature appears estimate sparsity adapts three different zones estimation first signal sparse enough thresholding benefit second appropriately chosen threshold substantially improved estimation third signal sparse zero estimate gives optimum accuracy rate let y beta epsilon y n x vector observations beta p x vector unknown regression coefficients n x p design matrix e spherically symmetric error term unknown scale parameter consider estimation general quadratic loss functions particular extend work strawderman j amer statist assoc casella ann statist j amer statist assoc finding adaptive minimax estimators nonnality assumption generalized bayes beta greater numerical stability e smaller condition number usual least squares estimator particular give subclass estimators surprisingly simple form certain conditions generalized bayes minimax estimators normal case generalized bayes minimax general case spherically symmetric errors develops describes concerning disintegrations poisson random measures fashioned simple tools tailor made address inferential questions arising wide range bayesian nonparametric spatial statistical models poisson disintegration method based formal statement two concerning laplace functional change measure poisson palm fubini calculus terms random partitions integers n techniques analogous much general techniques dirichlet process weighted gamma process developed ann statist ann inst statist math order illustrate flexibility approach large classes random probability measures random hazards intensities expressed functionals poisson random measures described describe unified posterior analysis classes discrete random probability identifies exploits features common models analysis circumvents many difficult issues involved bayesian nonparametric calculus including combinatorial component allows one focus unique features process characterized via real valued functions h applicability technique illustrated obtaining explicit posterior expressions levy cox moving average processes within general setting multiplicative intensity models addition novel computational procedures similar efficient procedures developed dirichlet process briefly discussed models consider time series model involving fractional stochastic component whose integration order lie stationary invertible nonstationary regions unknown additive deterministic component consisting generalized polynomial model thus incorporate competing descriptions trending behavior stationary input stochastic component parametric autocorrelation innovation distribution unknown form model thus semiparametric develop estimates parametric component asymptotically normal achieve m estimation efficiency bound equal found work adaptive lam lan approach major technical feature treat effect truncating autoregressive representation order form innovation proxies relevant innovation density parameterized result case semiparametric estimates employ nonparametric series estimation avoids complications conditions kernel approaches featured much work adaptive estimation time series models work thus contributes methods theory nonfractional time series models autoregressive moving averages monte carlo finite sample performance semiparametric estimates included consider estimation location pole memory parameter lambda alpha respectively covariance stationary linear processes whose spectral density function f lambda satisfies f lambda similar c vertical bar lambda lambda vertical bar alpha neighborhood lambda define consistent estimator lambda derive limit distribution z lambda related optimization problems true parameter value lie boundary parameter space z lambda distributed normal random variable lambda element pi whereas lambda pi z lambda mixture discrete continuous random variables weights equal specifically lambda z lambda distributed normal random variable truncated zero moreover describe examine two step estimator memory parameter alpha showing neither limit distribution rate convergence affected estimation lambda thus reinforce extend previous respect estimation alpha lambda assumed known priori small monte carlo included illustrate finite sample performance estimators exact form local whittle likelihood studied intent developing general purpose estimation procedure memory parameter d rely tapering differencing prefilters resulting exact local whittle estimator consistent n limit distribution values d optimization covers interval width less initial value process known establish bahadur representation sample quantiles linear widely used nonlinear processes local fluctuations empirical processes discussed applications trimmed winsorized means given extend previous ones establishing sharper bounds milder conditions thus new insight theory empirical processes dependent random variables problem ignorability likelihood based inference incomplete categorical data two versions coarsened random assumption car distinguished compatibility parameter distinctness assumption investigated several conditions ignorability require extra parameter distinctness assumption established car assumptions quite different implications depending whether underlying complete data model saturated parametric latter case car assumptions become inconsistent observed data concept breakdown point introduced hampel ph d dissertation univ california berkeley ann math statist developed among others huber robust new york donoho huber festschrift erich l lehmann wadsworth belmont ca proved successful context location scale regression problems attempts extend concept situations met general acceptance argue connected fact location scale regression problems translation affine groups give rise definition equivariance statistical functionals comparisons terms breakdown points seem useful restricted equivariant functionals even connection breakdown equivariance tenuous one consider construction optimal tests equivalence hypotheses specifically assume x x n d distribution p theta theta element r k let g theta real valued parameter interest null asserts g theta element b versus alternative g theta element b example hypotheses occur bioequivalence one may wish two drugs brand name proposed generic version therapeutic effect little optimal theory available testing problems purpose asymptotic optimality theory thus asymptotic upper bounds achievable well asymptotically uniformly powerful test constructions attain bounds asymptotic theory based le cam notion asymptotically normal experiments order approximate general problem limiting normal problem ump equivalence test obtained testing mean multivariate normal mean goodness fit testing single index models large sample behavior certain score type test investigated product obtain asymptotically distribution free maximin tests large class local alternatives furthermore characteristic function based goodness fit tests proposed omnibus able detect peak alternatives simulation indicate approximation limit distribution acceptable already moderate sample sizes applications two real data sets illustrated consider multiple testing problem testing k null hypotheses unknown family distributions assumed satisfy certain monotonicity assumption attention restricted procedures control familywise error rate strong sense satisfy monotonicity condition assumptions certain maximin optimality well known stepdown stepup procedures test null hazard rate monotone nondecreasing versus alternative proposed test statistic means calibrating new unlike previous approaches neither based assumption null distribution exponential instead empirical information used effectively identify eliminate consideration parts line hazard rate clearly increasing confine subsequent attention parts remain produces test greater apparent power without excessive conservatism exponential based tests approach calibration borrows ideas used certain tests unimodality density bandwidth increased distribution desired properties obtained test statistic involve smoothing fact based directly assessment convexity distribution function conventional empirical distribution test optimal power properties difficult cases called upon detect small departure form bump monotonicity general theoretical properties test numerical performance explored h h usual approach dealing multiplicity problem restrict attention procedures control familywise error rate fwer probability even one false rejection many applications particularly large one might willing tolerate one false rejection provided number cases controlled thereby increasing ability procedure detect false null hypotheses suggests replacing control fwer controlling probability k false rejections call k fwer derive single step stepdown procedures control k fwer without making assumptions concerning dependence structure p values individual tests particular derive stepdown procedure quite simple apply cannot improved without violation control k fwer consider false discovery proportion fdp defined number false rejections divided total number rejections defined rejections false discovery rate proposed benjamini hochberg j roy statist soc ser b controls e fdp construct methods gamma alpha p fdp gamma alpha two stepdown methods proposed first holds mild conditions dependence structure p values second conservative holds without dependence assumptions explore theoretical foundations twenty questions approach pattern recognition object analysis computational process rather probability distributions bayesian inference decision boundaries statistical learning formulation motivated applications scene interpretation great many possible explanations data one background statistically dominant imperative restrict intensive computation genuinely ambiguous regions focus pattern filtering given large set y possible patterns explanations narrow true one y small random subset y cap subset y detected patterns subjected intense processing end consider family tests y element versus nonspecific alternatives y element c test null type error candidate sets subset y arranged hierarchy nested partitions tests characterized scope vertical bar vertical bar power type ii error algorithmic cost consider sequential testing strategies decisions made iteratively based past outcomes test perform next stop testing set y cap taken set patterns ruled tests performed total cost strategy sum testing cost postprocessing cost proportional vertical bar y cap vertical bar corresponding optimization problem analyzed might expected mild assumptions good designs sequential testing strategies exhibit steady progression broad scope coupled power power coupled dedication specific explanations assumptions ensuring property key role played ratio cost power ideas illustrated context detecting rectangles amidst clutter consider problem adaptation margin binary classification suggest penalized empirical risk minimization classifier adaptively attains logarithmic factor fast optimal rates convergence excess risk rates faster n n sample size method gives adaptive estimators problem edge estimation develops nonparametric inference procedures estimation testing problems means manifolds central limit theorem frechet sample means derived leading asymptotic distribution theory intrinsic sample means riemannian manifolds central limit theorems obtained extrinsic sample means w r arbitrary embedding differentiable manifold euclidean space bootstrap methods particularly suitable problems presented applications given distributions sphere sal directional spaces real projective space rp n axial spaces complex projective space cpk planar shape spaces w r veronese whitney embeddings threedimensional shape space sigma smooth backfitting introduced marnmen linton nielsen ann statist promising technique fit additive regression models known achieve oracle efficiency bound propose discuss three fully automated bandwidth selection methods smooth backfitting additive models first one penalized least squares approach based order stochastic expansions residual sums squares smooth backfitting estimates two plug bandwidth selectors rely approximations average squared errors whose utility restricted local linear fitting large sample properties bandwidth selection methods given finite sample properties compared simulation experiments due curse dimensionality estimation multidimensional nonparametric regression model general feasible hence additional restrictions introduced additive model takes prominent place restrictions imposed lead serious bias new estimator proposed allows penalizing nonadditive part regression function offers smooth choice full additive model byproduct penalty leads regularization sparse regions additive model hold small penalty introduces additional bias compared full model compensated reduced bias due smaller bandwidths increasing penalties estimator converges additive smooth backfitting estimator mammen linton nielsen ann statist structure estimator investigated two algorithms provided proposal selection tuning parameters made respective properties studied finite sample evaluation performed simulated ozone data deals nonparametric shape respecting estimation method u shaped unimodal functions general upper bound nonasymptotic l risk estimator given method applied shape respecting estimation several classical functions among typical intensity functions encountered reliability field case derive upper bound spatially adaptive property estimator respect l metric approximately behaves best variable binwidth histogram function estimation mixed effect models widely used analysis correlated data longitudinal data repeated measures approach nonparametric estimation mixed effect models consider models parametric random effects flexible fixed effects employ penalized least squares method estimate models issue addressed selection smoothing parameters generalized cross validation method yield optimal smoothing real latent random effects simulation conducted investigate empirical performance generalized cross validation context real data examples presented applications methodology introduce general method uniform bandwidth consistency kernel type function estimators examples include kernel density estimator nadaraya watson regression estimator conditional empirical process may useful establish uniform consistency data driven bandwidth kernel type function estimators motivated applications prediction forecasting suggest methods approximating conditional distribution function random variable y given dependent random d vector x idea estimate distribution y vertical bar x y vertical bar theta x unit vector theta selected approximation optimal least squares criterion theta may estimated root n consistently furthermore estimation conditional distribution function y given theta x first order asymptotic properties would enjoy theta known proposed method illustrated simulated real data examples showing effectiveness independent datasets data time series numerical work corroborates theoretical result theta estimated particularly accurately suppose process yields independent observations whose distributions belong family parameterized theta e theta process control observations d known parameter value theta process control parameter changes apply idea robbins siegmund proc sixth berkeley symp math statist probab construct class sequential tests detection schemes whereby unknown post change parameters estimated approach especially useful situations parametric space intricate mixture type rules operationally conceptually difficult formulate exemplify approach applying problem detecting change shape parameter gamma distribution univariate multivariate setting relevance weighted likelihood introduced formally embrace variety statistical procedures trade bias precision unlike classical counterpart weighted likelihood combines relevant information inheriting many desirable features including good asymptotic properties order effective weights involved construction need judiciously chosen choosing weights cross validation resulting weighted likelihood estimator wle weakly consistent asymptotically normal application disease mapping data demonstrated many public health problems goal identify effect treatment intervention risk failure whole population marginal proportional hazards regression model often used analyze effect dependent censoring explained many auxiliary covariates utilize two working models condense dimensional covariates achieve dimension reduction estimator treatment effect obtained maximizing pseudo likelihood function sieve space estimator consistent asymptotically normal either two working models correct additionally working models correct asymptotic variance semiparametric efficiency bound consider marginal models liang zeger bioinetrika analysis longitudinal data develop theory statistical inference models existence weak consistency asymptotic normality sequence estimators defined roots pseudo likelihood equations goal describe application quasi likelihood estimating equations spatially correlated binary data logistic function used model marginal probability binary responses terms parameters interest mild assumptions correlations leonov shiryaev formula combined comparison characteristic functions used establish asymptotic normality linear combinations binary responses consistency asymptotic normality quasi likelihood estimates derived modeling spatial correlation variogram apply asymptotic test independence two spatially correlated binary outcomes illustrate concepts well known example based data lansing woods comparison generalized estimating equations proposed approach discussed motivated statistical evaluation complex computer models deal issue objective prior specification parameters gaussian processes particular derive jeffreys rule independence jeffreys reference priors situation resulting posterior distributions proper quite general set conditions proper flat prior strategy based maximum likelihood estimates considered priors compared grounds frequentist properties ensuing bayesian procedures computational issues addressed illustrate proposed solutions means example taken field complex computer model validation frequent well founded criticism maximum posteriori map minimum mean squared error mmse estimates continuous parameter gamma taking values differentiable manifold f invariant arbitrary reparameterizations clarifies issues surrounding problem pointing difference coordinate invariance sine qua non mathematically well defined problem diffeomorphism invariance substantial issue solution first presence metric structure f used define coordinate invariant map mmse estimates argue natural way proceed discuss choice metric structure f imposing invariance criterion natural within bayesian framework choice essentially unique necessarily correspond choice coordinates cases complete prior ignorance jeffreys prior used invariant map estimate reduces maximum likelihood estimate invariant map estimate coincides minimum message length mml estimate discretization approximation used derivation hierarchical modeling wonderful stay hyperparameter priors often chosen casual fashion unfortunately number hyperparameters grows effects casual choices multiply leading considerably inferior performance extreme uncommon example wrong hyperparameter priors even lead impropriety posterior exchangeable hierarchical multivariate normal models first determine standard class hierarchical priors proper improper posteriors next determine elements class lead admissible estimators mean quadratic loss considerations one useful guideline choice among hierarchical priors computational issues resulting posterior distributions addressed suppose p theta g linear functional dirichlet process shape theta h theta total mass h fixed probability measure describes one well known bayesian prior posterior analysis dirichlet process posterior calculus gamma processes ascertain properties linear functionals dirichlet processes particular conjunction gamma identity easily generalized cauchy stieltjes transform linear functional dirichlet process equivalent laplace functional class define beta gamma processes represents generalization identity due cifarelli regazzini known markov krein identity mean functionals dirichlet processes new explanations interpretations identities analogues quite useful identities beta gamma random variables give result used ascertain specifications h dirichlet functional beta distributed avoids need inversion formula cases points special nature dirichlet process indeed functional beta gamma calculus developed antithetic coupling general stratification strategy reducing monte carlo variance without increasing simulation size antithetic principle monte carlo typically employs two strata via antithetic quantile coupling stratification obtained k e g k antithetically coupled variates offer substantial additional gain monte carlo efficiency terms variance bias reason reduced bias antithetically coupled chains dispersed search state space multiple independent chains emerging area perfect simulation perfect setting implementing k process parallel antithetic coupling mcmc without antithetic coupling class methods delivers genuine independent draws furthermore antithetic backward coupling convenient theoretical tool investigating antithetic forward coupling generation k antithetic variates negatively associated preserve negative correlation monotone transformations extremely antithetic negatively correlated possible complicated compared case k establish theoretical framework investigating issues among generating methods compare latin hypercube sampling iterative extension appear general purpose choices making another direct link monte carlo quasi monte carlo construct nonparametric confidence sets regression functions wavelets uniform besov balls consider thresholding modulation estimators wavelet coefficients confidence set obtained showing pivot process constructed loss function converges uniformly mean zero gaussian process inverting pivot yields confidence set wavelet coefficients obtain confidence sets functionals regression curve variable selection linear regression model takes many apparent faces frequentist bayesian standpoints introduce variable selection method referred rescaled spike slab model importance prior hierarchical specifications draw connections frequentist generalized ridge regression estimation specifically usefulness continuous bimodal priors model hypervariance parameters effect scaling posterior mean relationship penalization several model selection strategies frequentist bayesian nature developed studied theoretically importance selective shrinkage effective variable selection terms risk misclassification achieved posterior rescaled spike slab model verify procedure ability reduce model uncertainty finite samples specialized forward selection strategy tool illustrate effectiveness rescaled spike slab models reducing model uncertainty propose generalized functional linear regression model regression situation response variable scalar predictor random function linear predictor obtained forming scalar product predictor function smooth parameter function expected value response related linear predictor via link function addition variance function specified leads functional estimating equation corresponds maximizing functional quasi likelihood general approach includes special cases functional linear model well functional poisson regression functional binomial regression latter leads procedures classification discrimination stochastic processes functional data consider situation link variance functions unknown estimated nonparametrically data semiparametric quasi likelihood procedure essential step proposal dimension reduction approximating predictor processes truncated karhunen loeve expansion develop asymptotic inference proposed class generalized regression models proposed asymptotic approach truncation parameter increases sample size martingale central limit theorem applied establish resulting increasing dimension asymptotics establish asymptotic normality properly scaled distance estimated true functions corresponds suitable l metric defined generalized covariance operator consequence obtain asymptotic tests simultaneous confidence bands parameter function determines model proposed estimation inference classification procedures variants unknown link variance functions investigated simulation practical selection number components works well aic criterion finding supported theoretical considerations include application classification medflies regarding remaining longevity status based observed initial egg laying curve female medflies quantile regression tool estimation conditional quantiles response y given vector covariates x used measure effect covariates center distribution upper tails develops theory quantile regression tails specifically obtains large sample properties extremal extreme order intermediate order quantile regression estimators linear quantile regression model tails restricted domain minimum attraction closed tail equivalence across regressor values modeling setup combines restrictions extreme value theory leading homoscedastic heteroscedastic linear specifications regression analysis large samples extreme order regression quantiles converge weakly argmin functionals stochastic integrals poisson processes depend regressors intermediate regression quantiles functionals converge normal vectors variance matrices dependent tail parameters regressor design propose new data driven smooth tests parametric regression function smoothing parameter selected new criterion favors large smoothing parameter null resulting test adaptive rate optimal consistent pitman local alternatives approaching parametric model rate arbitrarily close root n asymptotic critical values come standard normal distribution bootstrap used small samples general formalization allows one consider large class linear smoothing methods tailored detection additive alternatives let n independent necessarily identically distributed bernoulli random variables let x n sigma j n j v bounded region local central limit theorem expansion p x n exn v developed given degree conditioning expansion information order correlation structure dependent weighted sampling schemes population e special case simple random sampling set d c e sampled probability proportional pi element d x x positive weights associated individuals element e used determine asymptotic information consistency asymptotic normality conditional unconditional logistic likelihood estimator unmatched case control designs sets controls size sampled equal probability deals exclusively crossover designs purpose comparing test treatments control treatment number periods larger among specifies sufficient conditions crossover design simultaneously optimal mv optimal large appealing class crossover designs expected optimal designs highly efficient entire class crossover designs computationally useful tools given used build assorted small optimal efficient crossover designs model robustness newly discovered crossover designs discussed minimum aberration increasingly popular criterion comparing assessing fractional factorial designs would question importance usefulness nowadays past decade great deal work done minimum aberration various extensions develops general theory minimum aberration based sound statistical principle theory unified framework minimum aberration extends existing work area importantly theory offers systematic method enables experimenters derive aberration criteria general theory brings together two seemingly separate research areas one minimum aberration designs designs requirement sets facilitate design construction develop complementary design theory quite general class aberration criteria immediate application present construction weak version class criteria estimates chernoff rate efficiency quantum testing joint separate measurements approximate bounds rate given states mixed exact expressions derived least one states pure efficiencies tests separate joint measurements compared illustrated test quantum entanglement analysis variance anova extremely method exploratory confirmatory data analysis unfortunately complex problems e g split plot designs always easy set appropriate anova propose hierarchical analysis automatically gives correct anova comparisons even complex scenarios inferences means variances performed model separate batch effects row anova table connect classical anova working finite sample variance components fixed random effects models characterized inferences existing levels factor new levels respectively introduce new graphical display showing inferences standard deviations batch effects illustrate two examples applied data analysis first illustrating usefulness hierarchical computations displays second showing ideas anova helpful understanding previously fit hierarchical model many statistical problems stochastic signals represented sequence noisy wavelet coefficients develop general empirical bayes methods estimation true signal estimators approximate certain oracle separable rules achieve adaptation ideal risks exact minimax risks broad collections classes signals particular estimators uniformly adaptive minimum risk separable estimators exact minimax risks simultaneously besov balls smoothness shape indices uniformly superefficient convergence rates compact sets besov spaces finite secondary shape parameter furthermore classes nested besov balls smoothness index estimators dominate threshold james stein estimators within infinitesimal fraction minimax risks general block empirical bayes estimators developed white noise drift nonparametric regression ire considered many statistical practices involve choosing full model reduced models coefficients reduced zero data used select model estimated coefficients possible still come estimator always better traditional estimator based full model james stein estimator estimator property called minimaxity estimator considers one reduced model namely origin hence reduces coefficient estimator zero every coefficient estimator zero many applications including wavelet analysis desirable reduce zero estimators smaller threshold called thresholding possible construct kind estimators minimax construct minimax estimators perform thresholding apply recommended estimator wavelet analysis performs best among well known estimators aiming simultaneously estimation model selection estimators asymptotically optimal resurgence interest multiple testing occurred last decade motivated genomics microarrays dna sequencing drug screening clinical trials bioassays education psychology statisticians devoting considerable research energy effort properly analyze multiple endpoint data response new applications new criteria new methodology many ad hoc procedures emerged classical requirement procedures control strong familywise error rate fwe predetermined level probability false rejection true null less equal alpha finding desirable powerful multiple test procedures difficult requirement one recent ideas concerned controlling false discovery rate fdr expected proportion rejected hypotheses fact true many multiple test procedures control fdr much earlier approach multiple testing formulated lehmann ann math statist lehmann approach decision theoretic treats multiple endpoints problem k finite action problem k endpoints approach appealing since unlike fwe fdr criteria finite action approach pays attention false acceptances well false rejections view multiple endpoints problem k finite action problem popular procedures single step step step tip front point view admissibility bayes limit bayes properties model prototypical one loss function able following fairly general conditions specified single step procedure admissible ii sequence prior distributions given step procedure limit sequence bayes procedures iii vector risk function component risk individual testing problem various admissibility inadmissibility obtained companion cohen sackrowit ann statist able give characterization bayes procedures limits characterization yields complete class additional useful result step procedure inadmissible inadmissibility step demonstrated stringent loss function additional decision theoretic type obtained problem multiple endpoint testing k endpoints treated k finite action problem loss function chosen vector loss function consisting two components two components lead vector risk one component vector risk false rejection rate frr expected number false rejections component false acceptance rate far expected number acceptances corresponding null false loss function stringent positive linear combination loss function lehmann ann math statist cohen sackrowitz ann statist sense class admissible rules larger vector risk formulation linear combination risk function words fewer procedures inadmissible vector risk formulation statistical model assumed vector variables z multivariate normal mean vector known intraclass covariance matrix e endpoint hypotheses h mu vs k mu k characterization symmetric bayes procedures limits obtained characterization leads complete class theorem complete class theorem used useful necessary condition admissibility procedure main result step multiple endpoint procedure inadmissible develops new methodology together related theories combining information independent confidence distributions formal definition confidence distribution asymptotic counterpart e asymptotic confidence distribution given illustrated context combining information two general combination methods developed first along lines combining p values notable differences regard optimality bahadur type efficiency second multiplying normalizing confidence densities latter approach inspired common approach multiplying likelihood functions combining parametric information develops adaptive combining methods supporting asymptotic theory practical interest key point adaptive development methods attempt combine correct information downweighting excluding containing little wrong information true parameter interest combination methodologies illustrated simulated real data examples variety applications theory superefficiency adaptation developed flexible performance measures give multiresolution view risk bridge gap pointwise global estimation theory useful benchmark evaluation spatially adaptive estimators possible degree superefficiency minimax rate optimal estimators critically depends size neighborhood risk measured wavelet procedures given adapt rate optimally given shrinking neighborhoods including extreme cases mean squared error point mean integrated squared error whole interval adaptive procedures based new wavelet block thresholding scheme combines commonly used horizontal blocking wavelet coefficients resolution level vertical blocking coefficients across different resolution levels propose general methodology based multiple testing testing mean gaussian vector r n belongs convex set test achieves nominal level characterize class vectors tests achieve prescribed power functional regression model general methodology applied test qualitative hypotheses oil regression function example est regression function positive increasing convex generally satisfies differential inequality uniform separation rates classes smooth functions established comparison provided simulation evaluates procedures testing monotonicity propose general series method estimate semiparametric partially linear varying coefficient model establish consistency root n normality property estimator finite dimensional parameters model error conditionally homoskedastic estimator semiparametrically efficient sense inverse asymptotic variance estimator finite dimensional parameter reaches semiparametric efficiency bound model small scale simulation reported examine finite sample performance proposed estimator empirical application presented illustrate usefulness proposed method practice discuss obtain efficient estimation result error conditional heteroskedastic kernel based classification univariate distributions two populations optimal bandwidth choice dichotomous character two densities cross one point curvature signs minimum bayes risk achieved bandwidths order magnitude larger minimize pointwise estimation error hand curvature signs different multiple crossing points bandwidths conventional size generally appropriate range different modes behavior narrower multivariate settings optimal size bandwidth generally appropriate pointwise density estimation properties motivate empirical rules bandwidth choice consider semiparametric model euclidean parameter infinite dimensional parameter called banach parameter assume exists efficient estimator euclidean parameter b value euclidean parameter known exists estimator banach parameter depends value efficient within restricted model substituting efficient estimator euclidean parameter value parameter estimator banach parameter one obtains efficient estimator banach parameter full semiparametric model euclidean parameter unknown hereditary property efficiency completes estimation semiparametric models euclidean parameter estimated efficiently typically estimation euclidean banach parameter necessary order describe random phenomenon sufficient extent since efficient estimators asymptotically linear substitution method particular case substituting asymptotically linear estimators euclidean parameter estimators asymptotically linear depend euclidean parameter general substitution case studied sake well hereditary property asymptotic linearity proved let given contaminated list n r d valued observations coming g different normally distributed populations common covariance matrix compute ml estimator respect certain statistical model n r outliers parameters g populations detects outliers simultaneously partitions complement g clusters turns estimator unites minimum covariance determinant rejection method well known pooled determinant criterion cluster analysis propose efficient algorithm approximating estimator breakdown points mean values pooled ssp matrix general depth weighted scatter estimators introduced investigated general depth functions affine equivariant scatter estimators fisher consistent unbiased wide range multivariate distributions sample scatter estimators strong root n consistent asymptotically normal influence functions estimators exist bounded general concentrate specific case general depth weighted scatter estimators projection depth weighted scatter estimators include special case well known stahel donoho scatter estimator whose limiting distribution long open large sample behavior including consistency asymptotic normality efficiency finite sample behavior including breakdown point relative efficiency sample projection depth weighted scatter estimators thoroughly investigated influence function maximum bias projection depth weighted scatter estimators derived examined unlike typical breakdown competitors projection depth weighted scatter estimators integrate breakdown point efficiency enjoying bounded influence function moderate maximum bias curve comparisons leading estimators asymptotic relative efficiency gross error sensitivity reveal projection depth weighted scatter estimators behave well overall consequently represent favorable choices affine equivariant multivariate scatter estimators introduce generalized bootstrap technique estimators obtained solving estimating equations special cases generalized bootstrap classical bootstrap efron delete d jackknife variations bayesian bootstrap proposed technique discussed examples distributional consistency method established asymptotic representation resampling variance estimator obtained investigates effects smoothed bootstrap iterations coverage probabilities smoothed bootstrap bootstrap confidence intervals population quantiles establishes optimal kernel bandwidths various stages smoothing procedures conventional smoothed bootstrap bootstrap methods known yield onesided coverage errors orders o n o n respectively intervals based sample quantile random sample size n sharpen latter result o n proper choices bandwidths bootstrapping studentization steps calibration nominal coverage level means iterated bootstrap succeeds reducing coverage error smoothed bootstrap percentile interval order o n smoothed bootstrap interval o n provided bandwidths selected appropriate orders simulation confirm asymptotic suggesting iterated smoothed bootstrap method yields accurate coverage hand iterated smoothed bootstrap percentile method interval advantage shorter stable bootstrap intervals
