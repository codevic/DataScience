corsim microsimulator vehicular traffic studied respect ability successfully model predict behavior traffic block section chicago inputs simulator include information street configuration driver behavior traffic light timing turning probabilities intersection distributions traffic ingress system data available concerning turning proportions actual neighborhood well counts vehicular ingress neighborhood internal system counts day may data accurate video recordings quite inaccurate observer counts vehicles previous full dataset involved tuning parameters corsim ad hoc fashion corsim output reasonably close actual data common approach simply tuning complex computer model real data result poor parameter choices completely ignores often considerable uncertainty remaining parameters overcome problems adopt bayesian approach together measurement error model inaccurate data derive posterior distribution turning probabilities parameters corsim input distribution posterior distribution used initialize runs corsim yielding outputs reflecting actual uncertainty analysis determining posterior via markov chain monte carlo mcmc methodology directly feasible running time corsim fortunately turning probabilities parameters input distribution enter corsim probability structure almost exactly described stochastic network allow mcmc analysis resulting mcmc novel features useful dealing general discrete network structures major possible incorporate uncertainty model inputs analyses traffic microsimulators corsim incorporating uncertainty significantly change variability engineering simulations performed corsim second engineering traffic counts obtained human observers significant bias positive direction corresponding overcounting vehicles unexpected information origin destination od matrix transport network fundamental requirement much transportation planning relatively inexpensive method updating od matrix draw inference od matrix based single observation traffic flows specific set network links bayesian approach natural choice combining prior knowledge od matrix current observation traffic flows existing approaches bayesian modeling od matrices include normal approximations poisson distributions leads posterior intractable even simple special cases markov chain monte carlo simulation incurs extreme demand computational efforts em algorithm bayesian inference reinvestigated transport network estimating population means traffic flows reconstructing traffic flows predicting future traffic flows resultant estimates simple forms minimal computational costs traditional shewhart control charts usually considered effective detecting large shifts process parameters ineffective detecting small shifts detecting small parameter shifts much better exponentially weighted moving average ewma control charts cumulative sum cusum control charts charts considered effective shewhart charts large parameter shifts frequently recommended ewma cusum charts used combination shewhart chart gain benefits types charts small large shifts detected quickly consider problem process monitoring continuous process variable observed objective detect small large shifts either process mean mu process standard deviation sigma situation customary combination two control charts one chart designed monitor mu designed monitor sigma situation best ewma cusum chart monitoring mu based sample means best chart monitoring based squared deviations target ewma cusum chart combination based sample means squared deviations target effective detecting small large shifts mu alpha type combination effective terms overall performance combinations include chart based squared deviations target generally least effective combinations include shewhart chart thus conclude really necessary shewhart chart ewma cusum chart obtain best overall performance necessary ewma cusum chart based squared deviations target many types control charts ability detect process changes weaken time depending past data observed often referred inertia problem propose new measure inertia signal resistance largest standardized deviation target leading immediate control signal calculate signal resistance values several types univariate multivariate charts conclusions support recommendation shewhart limits used exponentially weighted moving average charts especially smoothing parameter small moments run length distribution often used design performance quality control charts run length distribution chart monitoring multivariate process mean analyzed assumed control process observations iid random samples multivariate normal distribution unknown mean vector covariance matrix control run length distribution chart depend unknown process parameters furthermore control run length distribution chart depends statistical distance control control mean vectors follows performance analysis given without knowledge control values parameters estimates performance charts constructed traditional f distribution based control limits studied recommendations given sample size requirements necessary achieve desired performance corrected control limits given designing charts estimated parameters large sample sizes available propose model suitable statistical process control short production runs wish detect line whether mean process exceeded prespecified upper threshold value theoretical basis model bayesian formulation leading mixture normal distributions issues decisions whether process within specification forecasting addressed kalman filter model related special case model calculations illustrated clinical chemistry example tool wear problem another potential candidate approach competing risks model useful settings individuals units may die fail various causes case items cause failure known subgroup causes case say failure group masked widely used approach competing risks data without masking involves specification cause specific hazard rates often availability likelihood methods estimation testing piecewise constant hazards used piecewise constant rates offer model flexibility computational convenience piecewise constant hazard models choice endpoints interval hazards constant usually subjective one discuss propose model selection methods data driven automatic compare three model selection procedures based minimum description length principle bayes information criterion akaike information criterion fast splitting algorithm computational tool used select among enormous number possible models test effectiveness methods numerical including real dataset masked failure causes problem discussed arisen industrial scenario involving potential failure element building structures element carries warranty several scenario considered specific buildings occurs circumstances goes label product stewardship statistical content inference accelerated tests reverse testing stress levels making predictions conditions ones inference stress testing lifetimes well discussed topic statistical different three features time transformation functions relating parameters failure models stress material strength instead lifetimes metric reliability fusing information consisting test data expert testimonies degradation strength cumulative stress stress testing germane survival analysis immunity replacing strength scope work could extend biometry data used real including pertaining expert testimonies interest avoiding potential lawsuits owners data mandated neither source nature made public transformation sides nonlinear model often necessary obtain tractable error distributions dramatic effect optimum designs parameters model develops methods design experiments value power transformation parameter precisely known optimum designs particular transformations together efficiency designs transformation varies studied multivariate model suitable choice prior bayesian designs found robust correct transformation typical equispaced design evaluated properties designs studied various departures assumptions frequently stated advantage fractional factorial ff designs one factor time fat designs relative efficiency k factor k run designs k power divisible usually stated relative efficiency k favor resolution iv ff design orthogonal fat design examine ways measure efficiency main effects modeling case factors quantitative enables rescaling settings designs restricted lie smallest sized hypercubes hyperspheres equal volume relative efficiency ff design full model k effects active designs identical sense fat design rescaled rotated become resolution iv ff design primary advantage ff design lies projection properties k factors expected active equal volume designs efficiency ff least k k next taking size effects based approach rescale ff restricting design probability fat design producing large swings response e swings produce unusable runs efficiency ff usually remains greater reach k limit ideas may used help scale back originally planned settings fractional factorial design reduce chance unusable runs still maintaining advantages ff designs factors hard change others relatively easier split plot experiments often economic alternative fully randomized designs split plot experiments structure subplot arrays imbedded within whole plot arrays tendency become large particularly screening situations many factors considered alleviate problem explore case two level designs various ways orthogonal arrays plackett burman type reduce number individual tests general construction principles outlined resulting alias structure derived discussed advantages nonorthogonal resolution iv designs running small screening experiments primary goal identification main effects mes secondary goal entertaining small number potentially second order interactions accomplished evaluating structure performance designs obtained folding small efficient nonorthogonal resolution iii designs comparing commonly used orthogonal resolution iii designs comparable size fractional factorials plackett burman designs folded designs available wider class run sizes perform well better resolution iii competitors selecting correct model active two factor interactions present significantly outperform resolution iii competitors terms correctly identifying mes simple two step procedure proposed analyzing data designs separates goals well suited sorting likely models quickly multivariate regression models second order polynomial response surfaces proposed fitted surfaces response variable constrained expressed canonical forms features common common stationary points common sets eigenvectors greatly reduce number parameters required set surfaces easier interpret together cost greater computational burden automatic differentiation within package matlab easy reduce burden considerably describe models fit derive standard errors report small simulation application dataset develop asymptotic distribution eigenvalues quadratic response surface result eaton tyler extend carter chinchilli campbell bisgaard ankenman case including nondistinct eigenvalues particular suggest method estimating confidence regions multiple roots principal components analysis pca orthogonal regression deal finding p dimensional linear manifold minimizing scale orthogonal distances m dimensional data points manifold main conceptual difference pca p estimated data attain small proportion unexplained variability whereas orthogonal regression p equals m two main approaches robust pca eigenvectors robust covariance matrix searching projections maximize minimize robust univariate dispersion measure akin second approach rather finding components one one directly undertake problem finding given p p dimensional linear manifold minimizing robust scale orthogonal distances data points manifold scale may either smooth m scale trimmed scale iterative algorithm developed converge local minimum strategy based random search used approximate global minimum procedure faster breakdown point competitors especially large m case whereas p m yields orthogonal regression pca computationally efficient method choose p given comparisons based simulated real data proposed procedure robust competitors ridge analysis graphical inferential method exploring optimum factor levels response surface fixed distances center experimental region proposes approach ridge analysis optimizing response surface presence noise variables extend ridge analysis method peterson include factors noise variables approach allows investigator explore factor combinations mean squared error target value time keeping track much mean response differs target value allows investigator compute simultaneous confidence band root mean squared error target value guidance band aid determining optimal levels operation variety factor constraints imposed including found mixture experiments addition propose modification approach used larger better smaller better experiments illustrate proposed method two examples one mixture experiment biased regression br methods lukewarm approval largely practical difficulty data driven methods biased estimator better ordinary least squares ols nevertheless believe situations br methods worthy contenders example focus multivariate calibration chemical spectra goal predict component concentrations illustrate cautious application several br methods real simulated data keep ols solution unless strong evidence br solution better neural network modeling small datasets justified theoretical point view according bartlett showing generalization performance multilayer perceptron mlp depends l norm parallel c parallel weights hidden layer output layer rather total number weights investigate geometrical properties mlps drawing linear projection theory propose equivalent number degrees freedom used neural model selection criteria like akaike information criterion bayes information criterion unbiased estimation error variance measure proves much smaller total number parameters network usually adopted depend number input variables moreover concept compatible bartlett similar ideas long associated projection based models kernel models numerical involving real simulated datasets presented discussed various mixed models relevant analyzing army test data described along several testing interval estimation problems problems come context investigating gun tube accuracy m series tank particular tube tube dispersion factors affect tube tube variability might include tanks ammunition lot ammunition temperature firing occasions fixed factors others random factors inference problems arise tube tube dispersion somewhat different usually encountered typical anova situations unified approach solving problems presented concepts generalized p values generalized confidence intervals performance resulting tests confidence intervals numerically investigated found quite satisfactory analysis army test data presented illustrate develop procedures one two sided tolerance intervals normal general linear models exists set independent scaled chi squared random variables proposed procedures based concept generalized pivotal quantities applicable general mixed models provided balanced data available focuses situations involving unbalanced data specific attention given unbalanced one way random model generalized pivotal quantities allows construction tolerance intervals interest fairly straightforward practical examples given illustrate proposed procedures furthermore detailed statistical simulation conducted evaluate performance showing proposed procedures recommended practical optimal engineering design problem challenging nonlinear objective functions usually need evaluated dimensional design space presents data mining aided optimal design method able competitive design solution relatively computational cost method consists four components uniform coverage selection method chooses design representatives among large number original design alternatives nonrectangular design space feature functions evaluation computationally economical surrogate design objective function clustering method generates design library based evaluation feature functions instead objective function classification method create design selection rules eventually leading us competitive design components implemented facilitate optimal fixture layout design multistation panel assembly process benefit data mining aided optimal design clearly demonstrated comparison local optimization methods e g simplex search random search based optimizations e g simulated annealing propose new method selecting common subset explanatory variables aim model several response variables idea natural extension lasso technique proposed tibshirani based joint residual sum squares constraining parameter estimates lie within suitable polyhedral region properties resulting convex programming problem analyzed special case orthonormal design general case develop efficient interior point algorithm method illustrated dataset infrared spectrometry measurements qualitatively different correlated responses wavelengths aim select subset wavelengths suitable predictors many responses possible develops statistical methods useful comparing marginals f g bivariate distribution comparisons appropriate analysis matched pair data involving treatment control two treatments quantile comparison function q g f proposed lehmann generalized measure treatment effect fully randomized experiments applicable useful analysis matched pair data nonparametric simultaneous confidence bands q constructed asymptotic permutation methods copula bivariate distribution plays significant role methodology application illustrated dataset industrial experiment kriging popular analysis approach computer experiments purpose creating cheap compute meta model surrogate computationally expensive engineering simulation model maximum likelihood approach used estimate parameters kriging model likelihood function near optimum may flat situations leads maximum likelihood estimates parameters covariance matrix large variance overcome difficulty penalized likelihood approach proposed kriging model theoretical analysis empirical experience real world data suggest proposed method particularly context computationally intensive simulation model number simulation runs must kept small collection large sample set prohibitive proposed approach applied reduction piston slap unwanted engine noise due piston secondary motion issues related practical implementation proposed approach discussed nonregular fractional factorial designs plackett burman designs widely used industrial experiments run size economy flexibility novel criterion called moment aberration projection proposed rank classify nonregular designs measures goodness design moments number coincidences rows projection designs new criterion used rank classify designs runs examples given illustrate ranking designs supported design criteria propose algorithm incorporates combinatorial heuristic optimization methods generating d efficient factorial designs includes approach automatically construct orthogonal arrays including least one array every known specification fewer runs algorithm tries various initialization iteration methods chooses promising method applies computational effort chosen method algorithm orthogonal nearly orthogonal arrays accommodate restrictions interactions large designs usage requires minimal expertise input applications area optimal product design choice modeling marketing research discussed extend usual implementation u control charts uccs two ways first overcome restrictive often inadequate assumptions poisson model next eliminate need questionable base period sequential procedure empirical bayes eb bayes methods compare traditional frequentist implementation eb methods somewhat easy implement deal nicely extra poisson variability time informally check adequacy poisson assumption still need base period sequential full bayes approach hand avoids drawback traditional u charts implementation requires numerical simulation prior distribution several possibilities objective informative priors explored argue sequential full bayesian ucc powerful versatile tool process monitoring traditional approaches process optimization start fitting model optimizing model obtain optimal operating settings methods account uncertainty parameters model form model bayesian approaches proposed recently account uncertainty parameters model assuming model form known presents bayesian predictive approach process optimization accounts uncertainty model form accounting uncertainty parameters given potential model propose optimizing model averaged posterior predictive density response weighted average taken model posterior probabilities weights resulting model robust optimization illustrated two experiments one involving mixture experiment involving small composite design statistical process control spc involves ongoing checks ensure neither mean variability process readings changed conventionally done pairs charts shewhart x bar r charts cumulative sum charts mean variance exponentially weighted moving average charts mean variance traditional methods calculating statistical properties control charts based assumption control true mean variance known exactly assumed true values set center lines control limits decision intervals reality true parameter values seldom ever known exactly rather commonly estimated phase sample random errors estimates lead uncertain run length distribution resulting charts attractive alternative traditional charting methods single chart unknown parameter likelihood ratio test change mean variance normally distributed data formulation gives single diagnostic detect shift mean variance rather two separate diagnostics unknown parameter formulation recognizes reality best one reasonable estimates parameters exact values description implies immediate benefit formulation run behavior controlled despite lack large phase sample another benefit changepoint formulation competitive best traditional formulations detecting step changes parameters many intervention analysis applications time series data may expensive otherwise difficult collect case power function helpful used determine probability proposed intervention analysis application detect meaningful change assuming underlying autoregressive integrated moving average arima fractional arima model known estimated preintervention time series methodology computing required power function developed pulse step ramp interventions arima fractional arima errors convenient formulas computing power function special cases given illustrative applications traffic safety environmental impact assessment discussed reliability demonstration tests require demonstrating level confidence reliability exceeds given standard demonstration tests expensive time consuming careful planning sample size test length essential develops exact theoretical methods based pivotal quantities confidence intervals aid proper sample size selection determining long test run terms many units must fail test end demonstration tests failure censored type ii censored data log location scale corresponding location scale distributions methods implemented plus lognormal weibull log logistic distributions allow users develop graphs depicting probability successful demonstration function actual reliability target reliability sample size number units failing assumed distribution data ozone concentrations four monitoring sites central canada examined univariate extreme value methods allow required inference multivariate methods exploiting joint dependence data necessary families multivariate extreme value copulas flexible dependence structure closed form cumulative distribution functions recently derived maximum likelihood estimates parametric models may obtained numerically lead increased efficiency estimation margins assurance data well modeled distributions space multivariate extreme value copulas infinite dimensional data ozone levels prone outliers addresses robust methods required proper analysis transient markov chains sometimes simulated estimate rare event probabilities illustration chains defined airborne particle dispersion model used estimate probabilities released particles reach various locations estimated probabilities needed many purposes including exposure calculations affected populations optimization detector placement experimental designs simulation runs embedding fitted regression models output data importance sampling transition kernels convergence improved factors tens hundreds give enhancements several functional techniques forecast sulfur dioxide levels near power plant data considered time series curves assuming lag one dependence predictions computed functional kernel local bandwith linear autoregressive hilbertian model carry estimation called historical matrix subsample emphasizes uncommon shapes introduce bootstrap method evaluate range forecasts uses fraiman muniz order functional data compare functional techniques neural networks semiparametric methods former models often effective construct confidence bounds random effects calibration curve model example application analysis analytical chemistry data calibration curve contains measurements y several values known concentration x q laboratories laboratory considered random effect design intercept slope calibration curve allowed laboratory specific values develop appropriate interlaboratory calibration curve heteroscedastic data type commonly observed analytical chemistry b compute point estimate unknown true concentration x corresponding measured concentrations y y y q provided q laboratories e subset original q laboratories used calibrate model q q c compute asymptotic mean variance estimate d construct confidence region x illustrate methods simulated typical interlaboratory calibration data relevant applications general approach highlighted since seminal cook usual way measure influence observation statistical model delete observation sample compute convenient norm change parameters vector forecasts define new way measure influence observation based observation influenced rest data precisely new statistic propose defined squared norm vector changes forecast one observation sample points deleted one one new statistic asymptotically normal distribution able detect group leverage similar outliers undetected cook statistic several examples proposed statistic useful detecting heterogeneity regression models lame dimensional datasets propose general approach regression digitized multidimensional signals cart pose severe challenges standard statistical methods main contribution work build two dimensional coefficient surface allows interaction across indexing plane regressor array aim estimated coefficient surface reliable scalar prediction assume coefficients smooth alone indices present rather straightforward rich extension penalized signal regression penalized b spline tensor products appropriate difference penalties placed oil rows columns tenser product coefficients methods grounded standard penalized regression thus cross validation effective dimension diagnostics accessible model easily transplanted generalized linear model framework illustrative example motivates proposed methodology performance comparisons made popular methods compares three methods computing posterior probabilities live possible orders polynomial regression models posterior probabilities used forecasting bayesian model averaging bayesian model averaging closer relationship theoretical coverage density predictive interval hdpi observed coverage corresponding selecting best model performance different procedures illustrated simulations known engineering data consider linear regression model response variable may right censored standard maximum likelihood estimator mle based parametric approach estimation regression coefficients requires parametric form error distribution known given dataset may able valid parametric form error distribution case error distribution unknown arbitrary semiparametric approach plausible special modified semiparametric mle msmle regression coefficients proposed simulation suggests msmle consistent asymptotically normally distributed may efficient new procedure applied engineering data variable selection search j relevant predictor variables group p candidates standard problem regression analysis class id regression models broad class includes generalized linear models existing variable selection algorithms originally meant multiple linear regression based ordinary least squares mallows c p used id models graphical aids variable selection provided proposes new procedure analyzing small unreplicated factorial experiments procedure based likelihood ratio tests compare competing models easy method implementing procedure presented demonstrated real set data simulation presented indicate new procedure compares favorably lenth method tables constants supplied allow new procedure easily applied analysis run run run experiments introduce new method robust principal component analysis pca classical pca based empirical covariance matrix data hence highly sensitive outlying observations two robust approaches developed date first approach based eigenvectors robust scatter matrix minimum covariance determinant estimator limited relatively dimensional data second approach based projection pursuit handle dimensional data propose robpca approach combines projection pursuit ideas robust scatter matrix estimation robpca yields accurate estimates noncontaminated datasets robust estimates contaminated data robpca computed rapidly able detect exact fit situations product robpca produces diagnostic plot displays classifies outliers apply algorithm several datasets chemometrics engineering spatial spatiotemporal processes physical environmental biological sciences often exhibit complicated diverse patterns across different space time scales scientific understanding observational data vary form content across scales develop examine bayesian hierarchical framework combination information sources accomplished approach targeted settings various special spatial scales arise scales may dictated data collection methods availability prior information goals analysis approach restricts essential scales hence avoid challenging problem constructing model used scales means inferences preselected special scales problems involving special scales sufficiently common justify trade comparatively simple modeling analysis strategy formidable task forming models valid scales specifically approach based simple idea conditioning spatially continuous process areal average process resolution interest addition data prescribed resolutions conditioned areal averaged true process conditioning arguments fit nicely hierarchical bayesian framework methodology demonstrated spatial prediction quantity known streamfunction based wind information satellite observations weather center computer model output
