proposes adaptive monte carlo sampling schemes bayesian variable selection linear regression improve standard markov chain methods considering metropolis hastings proposals accumulated information posterior distribution obtained sampling adaptation needs done carefully ensure sampling correct ergodic distribution give conditions validity adaptive sampling scheme problem simulating distribution finite state space general suggest class adaptive proposal densities uses best linear prediction approximate gibbs sampler sampling scheme computationally much faster per iteration gibbs sampler taken account efficiency gains sampling scheme compared alternative approaches substantial terms precision estimation posterior quantities interest given amount computation time compare method sampling schemes examples involving real simulated data methodology developed extended variable selection general problems deal strong consistency bayesian density estimation awkward consequence inconsistency described pointed consistency density f depends prior mass assigned pathological set densities close f weak sense far apart f hellinger sense analysis sets leads identification notion data tracking specific examples phenomenon cannot occur discussed happen things go wrong thus providing intuition sources inconsistency covariance two variables multivariate gaussian distribution decomposed sum path weights paths connecting two variables undirected independence graph weights useful determining variables mediating correlation two path endpoints decomposition arises undirected gaussian graphical models require involve assumptions causality covariance decomposition derived basic linear algebra decomposition feasible large numbers variables corresponding precision matrix sparse circumstance arises examples gene expression functional genomics additional computational efficiences possible undirected graph derived acyclic directed graph typical alternative hypotheses analysis residuals standard regression model considered one bayesian diagnostic based symmetric form kullback leibler divergence determined include explicit expression diagnostic alternative errors generated unknown distribution function dirichlet process prior expression immediately interpretable exactly computable endowed asymptotic connections linear approximation diagnostic reveals close links class lagrange multiplier test alternative errors generated autoregressive process linear approximation proportional box pierce statistic ljung box statistic according characteristics prior observations zero mean depends durbin watson statistic errors first order autoregressive related cliff ord statistic generated first order spatial autoregression sensitivity prior diagnostic linear approximation discussed random sample curves usually thought noisy realisations compound stochastic process x z w z produces random amplitude variation w produces random dynamic phase variation applications estimate called structural mean mu e z crosssectional mean e x estimation problem difficult process z directly observable propose nonparametric maximum likelihood estimator mu estimator root n consistent asymptotically normal assumed model robust model misspecification simulations realdata example proposed estimator competitive landmark registration often considered benchmark advantage avoiding time consuming often infeasible individual landmark identification propose new estimator error variance nonparametric regression model estimate error variance intercept simple linear regression model squared differences paired observations dependent variable squared distances paired covariates regressor special case one dimensional domain equally spaced design points method reaches asymptotic optimal rate achieved existing methods conduct extensive simulations evaluate finite sample performance method compare existing methods method extended nonparametric regression models multivariate functions defined arbitrary subsets normed spaces possibly observed unequally spaced clustered designed points estimation finite population totals presence auxiliary information considered class estimators based penalised spline regression proposed estimators weighted linear combinations sample observations weights calibrated known control totals allow straightforward extensions multiple auxiliary variables complex designs standard design conditions estimators design consistent asymptotically normal admit consistent variance estimation familiar design based methods data driven penalty selection considered context unequal probability sampling designs simulation experiments estimators efficient parametric regression estimators parametric model incorrectly specified approximately efficient parametric specification correct example forest health monitoring survey data u forest service applicability methodology context two phase survey multiple auxiliary variables bivariate current status data univariate monitoring times identifiable part joint distribution three univariate cumulative distribution functions namely two marginal distributions bivariate cumulative distribution function evaluated diagonal smooth functionals univariate cumulative distribution functions efficiently estimated easily computed nonparametric maximum likelihood estimators based reduced data consisting univariate current status observations theory applied functionals address independence two survival times goodness fit copula model used wang ding brief simulations provided along illustration based data hiv transmission extension ideas incorporate covariates possibly time dependent discussed coherence conditions dose finding methods context phase clinical trials objective estimate targeted quantile unknown dose toxicity curve phase methods outcome adaptive thus escalate de escalate doses future patients based previous observations escalation new patient said coherent previous patient sign toxicity likewise de escalation coherent toxic outcome seen coherence conditions motivated ethical concerns trial conduct satisfied many statistical designs commonly used modifications methods examples coherence violated discusses coherence principles may applied calibrate two stage design deal situations delayed toxicity consider problem estimating regression coefficients competing risks model relationship cause specific hazard cause interest covariates described linear transformation models cause failure missing random subset individuals theory robins et al missing data problems approach chen et al estimating regression coefficients linear transformation models derive augmented inverse probability weighted complete case estimators regression coefficients doubly robust simulations relevance theory finite samples propose probabilistic bounds number false null hypotheses testing multiple hypotheses association simultaneously bounds valid general unknown dependence structures test power proposed estimator detect full proportion false null hypotheses discussed compared estimators proposed estimator deliver tight probabilistic bound number false null hypotheses multiple testing situation even strong dependence test discuss intrinsic autoregressions first order neighbourhood two dimensional rectangular lattice give exact formula variogram extends known asymmetric case obtain corresponding asymptotic expansion accurate general previous ones derive de wijs variogram appropriate averaging result interpreted two dimensional spatial analogue brownian motion obtained limit random walk one dimension bridge geostatistics de wijs process popular formulation markov random fields explains statistical analysis intrinsic autoregressions usually robust changes scale briefly describe corresponding calculations frequency domain including limiting order autoregressions closes practical considerations including applications irregularly spaced data two asymptotic frameworks increasing domain asymptotics infill asymptotics advanced obtaining limiting distributions maximum likelihood estimators covariance parameters gaussian spatial models without nugget effect limiting distributions known different cases interest know given finite sample framework appropriate consider possibility making choice basis well limiting distributions obtained framework approximate finite sample counterparts investigate quality approximations theoretically empirically showing certain consistently estimable parameters exponential covariograms approximations corresponding two frameworks perform equally well parameters cannot estimated consistently infill asymptotic approximation preferable traditional approach statistical inference identify true best model first little consideration specific goal inference model identification stage pursuit true model lead optimal regression estimation model selection well known bic consistent selecting true model aic minimax rate optimal estimating regression function recent promising direction adaptive model selection contrast aic bic penalty term data dependent theoretical empirical obtained support adaptive model selection still clear really share strengths aic bic model combining averaging attracted increasing attention means overcome model selection uncertainty bayesian model averaging optimal estimating regression function minimax sense answers questions basically negative model selection criterion consistent must behave suboptimally estimating regression function terms minimax rate covergence bayesian model averaging cannot minimax rate optimal regression estimation simple statistic proposed testing complete independence random variables multivariate normal distribution asymptotic null distribution statistic sample size number variables go infinity normal consequently test used number variables small relative sample size particular even number variables exceeds sample size finite sample size performance normal approximation evaluated simulation compared likelihood ratio test suppose two level hierarchical model distribution vector random parameters known estimated well data generated via fixed unobservable realisation vector derive smallest confidence region specific component random vector joint bayesian frequentist paradigm average optimal region much smaller corresponding bayesian highest posterior density region new estimation procedure especially appealing one deals data generated highly parallel structure new proposal illustrated dataset multi centre clinical one typical microarray experiment performance procedure examined via simulation concordance probability used evaluate discriminatory power predictive accuracy nonlinear statistical models derive analytical expression concordance probability cox proportional hazards model proposed estimator function regression parameters covariate distribution observed event censoring times reason asymptotically unbiased unlike harrell c index based informative pairs asymptotic distribution concordance probability estimate derived u statistic theory methodology applied predictive model lung cancer rubin schenker proposed approximate bayesian bootstrap two stage resampling procedure method creating multiple imputations missing data ignorable kim showed multiple imputation variance estimator biased moderate sample sizes method used reduce bias kim proposed modifying number samples drawn first stage bayesian bootstrap procedure note suggest alternative method reducing bias via simple correction factor applied standard multiple imputation variance estimate proposed correction easily implemented efficient procedure proposed kim simpler derivation sampling properties maximum likelihood estimators parameters inverse gaussian distribution submodels exponential family consider likelihood ratio tests hypotheses render parameters nonidentifiable first establish asymptotic equivalence likelihood ratio test score test secondly score test representation used derive asymptotic distribution likelihood ratio test derived general submodels exponential family without assuming compactness parameter space exemplify class multivariate normal models null hypotheses concerning covariance structure lead loss identifiability parameter motivating problem throughout test random intercepts model alternative covariance structure allowing serial correlation composite likelihood consists combination valid likelihood objects usually related small subsets data merit composite likelihood reduce computational complexity possible deal large datasets complex models even standard likelihood bayesian methods feasible aim suggest integrated general approach inference model selection composite likelihood methods particular introduce information criterion model selection based composite likelihood describe applications modelling time series counts dynamic generalised linear models analysis well known old faithful geyser dataset consider parametric frameworks prediction future values random variable y based previously observed data x simple pivotal methods obtaining calibrated prediction intervals presented illustrated frequentist predictive distributions defined confidence distributions utility demonstrated simple pivotal based approach produces prediction intervals predictive distributions well calibrated frequentist probability interpretations introduced efficient simulation methods producing predictive distributions considered properties related average kullback leibler measure goodness predictive estimated distributions given predictive distributions optimal certain settings invariance structure dominate plug distributions certain conditions propose novel approach estimating mean difference two highly skewed distributions method call smooth quantile ratio estimation smooths percentiles ratio quantiles two distributions method defines large class estimators including sample mean difference maximum likelihood estimator log normal samples l estimator derive asymptotic properties consistency asymptotic normality closed form expression asymptotic variance simulation smooth quantile ratio estimation mean squared error several competitors including sample mean difference log normal parametric estimator several realistic situations apply method national medicare expenditure survey estimate difference medical expenditures persons suffering smoking attributable diseases lung cancer chronic obstructive pulmonary disease persons without diseases modelling human genetic variation critical understanding genetic basis complex disease human genome project discovered millions binary dna sequence variants called single nucleotide polymorphisms millions may exist coding proteins takes place along chromosomes organisation polymorphisms along chromosome haplotype phase structure may discovering genetic variants associated disease haplotype phase often uncertain procedures model distribution parental haplotypes distribution misspecified lead substantial bias parameter estimates even complete genotype information available geometric approach estimation presence nuisance parameters address problem develop locally efficient estimators effect haplotypes disease robust incorrect estimates haplotype frequencies methods demonstrated simulation case parent design considers analysis current status data cured proportion population mixture model combines logistic regression formulation probability cure semiparametric regression model time occurrence event semiparametric regression model belongs flexible class partly linear models allows one explore possibly nonlinear effect certain covariate response variable sieve maximum likelihood estimation method proposed asymptotic properties proposed estimators discussed mild conditions estimators strongly consistent convergence rate estimator unknown smooth function obtained estimator unknown parameter asymptotically efficient normally distributed simulation carried investigate performance proposed method model fitted dataset calcification hydrogel intraocular lenses complication cataract treatment accelerated failure time model attractive alternative cox model proportionality assumption fails capture relationship survival time longitudinal covariates several complications arise covariates measured intermittently different time points different possibly measurement errors measurements available failure time joint modelling failure time longitudinal data offers solution complications explore joint modelling approach accelerated failure time assumption covariates assumed follow linear mixed effects model measurement errors procedure based maximising joint likelihood function random effects treated missing data monte carlo em algorithm used estimate unknown parameters including unknown baseline hazard function performance proposed procedure checked simulation case reproductive egg laying data female mediterranean fruit flies relationship longevity effectiveness new procedure treatment found effective clinical attention often focuses optimum efficacious treatment delivery treatment duration response optimum treatment delivery refers treatment length optimises mean response many treatment length often left discretion attending investigator physician may abruptly terminated treatment terminating events thus recommended treatment length often delineates treatment duration policy prescribes treatment given specified length time treatment terminating event occurs whichever comes first estimating functional relationship response treatment duration policy continuously time focus accelerated failure time model specifies logarithm failure time linearly related covariate vector without assuming parametric error distribution consider semiparametric box cox transformation model includes regression model special case analyse possibly censored failure time observations inference procedures transformation regression parameters proposed via resampling technique prediction survival function future specific covariate vector provided via pointwise simultaneous interval estimates proposals illustrated datasets two clinical bayesian adaptive design proposed comparative two armed clinical trial decision theoretic approaches loss function specified based cost patient costs making incorrect decisions end trial interim analysis decision terminate continue trial based expected loss function concurrently incorporating efficacy futility cost maximum number interim analyses determined adaptively observed data derive explicit connections loss function frequentist error rates desired frequentist properties maintained regulatory settings operating characteristics design evaluated frequentist grounds extensive simulations carried compare proposed design existing ones design general enough accommodate continuous discrete types data illustrate methods animal evaluating medical treatment cardiac arrest develops new semiparametric model effect covariates conditional intensity recurrent event counting process model transparent extension accelerated failure time model univariate survival data estimation regression parameter motivated semiparametric efficiency considerations extending class weighted log rank estimating functions originally proposed prentice subsequently studied detail tsiatis ritov novel rank based one step estimator regression parameter proposed aalen type estimator baseline intensity function obtained asymptotics handled empirical process methods finite sample properties studied via simulation new model applied bladder tumour data byar consider mixture models components data vectors given subpopulation statistically independent independent blocks argue condition independence take nonparametric view problem allow number subpopulations quite general distributions mixing proportions often estimated root n consistently indeed data k variate p subpopulations p minimal value k k p say mixture problem always nonparametrically identifiable distributions mixture proportions nonparametrically identifiable k k p treat case p detail construct explicit distribution density mixture proportion estimators converging conventional rates values p addressed similar approach although methodology becomes rapidly complex p increases present orthonormal bases approach detecting general differences among continuous distributions unknown density function represented finite vector estimated fourier coefficients respect suitable orthonormal basis wide class orthonormal bases establish asymptotic normality vector estimated fourier coefficients propose unbiased consistent estimator asymptotic covariance matrix fourier coeffients modelled functions fixed possibly random effects approach allows simultaneous detection distributional differences attributable various factors clustered correlated data suffciently large numbers observations per cluster fixed random effects realisations work motivated multi level clustered non gaussian datasets genetic recent peng yao gave interesting extension least absolute deviation estimation generalised autoregressive conditional heteroscedasticity garch time series models asymptotic distributions absolute residual autocorrelations squared residual autocorrelations garch model estimated least absolute deviation method derived lead two useful diagnostic tools used check whether garch model fitted least absolute deviation method adequate simulation experiments give support asymptotic theory real data example reported many applications researchers interested estimating mean multivariate normal random vector whose components order restrictions various authors demonstrated likelihood based methodology may perform poorly certain conditions problems problem much harder underlying covariance matrix nondiagonal simple iterative algorithm introduced used estimating mean multivariate normal population components order restriction proposed methodology illustrated application human reproductive hormone data hierarchical likelihood statistically efficient procedure frailty models recently method computationally attractive orthodox best linear unbiased predictor proposed uses pearson type estimation compare approaches discuss relative merits semiparametric frailty models difficulties arise orthodox method number nuisance parameters increases sample size difficulty avoided hierarchical likelihood method well known maximum likelihood fit logistic regression parameters greatly affected atypical observations several robust alternatives proposed consider model case control viewpoint clear current techniques exhibit poor behaviour many common situations new robust class estimation procedures introduced estimators constructed via minimum distance approach identifying model semiparametric biased sampling model approach developed case control sampling scheme yet applicable prospective sampling well weighted cramer von mises distance used illustrative example methodology derive monte carlo amenable minimum variance unbiased estimator nonlinear function normal mean variance estimator applications problems arising analysis data measured error described investigate mean squared error stein james estimator mean observations generated gaussian vector stationary process dimension greater two first assuming process short memory evaluate mean squared error compare sample mean sufficient condition stein james estimator improve upon sample mean given terms spectral density matrix around origin repeat analysis gaussian vector long memory processes numerical examples clearly illuminate stein james phenomenon dependent samples potential improve usual trend estimator time series regression models marginal likelihood conditional likelihood often used eliminating nuisance parameters parametric model well known full likelihood decomposed product conditional likelihood marginal likelihood property less transparent nonparametric semiparametric likelihood setting nice parametric likelihood property carried empirical likelihood world discuss applications case control genetical linkage analysis genetical quantitative traits analysis tuberculosis infection data unordered paired data treated semiparametric finite mixture models consider estimation problem detail simplest case unordered paired data observe minimum maximum values two random variables identities minimum maximum values lost profile empirical likelihood approach used maximum semiparametric likelihood estimation present large sample along simulation compare two samples censored data propose unified method semiparametric inference parameter interest model one sample parametric nonparametric parameter interest may represent example comparison means survival probabilities confidence interval derived semiparametric inference based empirical likelihood principle improves counterpart constructed common estimating equation empirical likelihood ratio asymptotically chi squared simulation experiments illustrate method based empirical likelihood substantially outperforms method based estimating equation real dataset analysed covariate effects considered multi state survival analysis dominated either parametric markov regression models semiparametric markov regression models cox proportional hazards models transition intensities states purpose research work alternatives cox model general finite state markov process setting shall look two alternative models aalen nonparametric additive hazards model lin ying semiparametric additive hazards model former allows effects covariates vary freely time latter assumes regression coefficients constant time basic tools product integral functional delta method present estimator transition probability matrix develop large sample theory estimator two models data hla identical sibling transplants acute leukaemia international bone marrow transplant registry serve illustration propose penalised pseudo partial likelihood method variable selection multivariate failure time data growing number regression coefficients certain regularity conditions consistency asymptotic normality penalised likelihood estimators certain penalty functions proper choices regularisation parameters resulting estimator correctly identify true model known advance based simple approximation penalty function proposed method easily carried newton raphson algorithm conduct extensive monte carlo simulation assess finite sample performance proposed procedures illustrate proposed method analysing dataset framingham heart centred gaussian model markov respect undirected graph g characterised parameter set precision matrices cone m g positive definite matrices entries corresponding missing edges g constrained equal zero bayesian framework conjugate family precision parameter distribution wishart density respect lebesgue measure restricted m g call distribution g wishart g nondecomposable normalising constant g wishart cannot computed closed form give simple monte carlo method computing normalising constant main feature method sampling distribution exact consists product independent univariate standard normal chi squared distributions read graph g computing normalising constant necessary obtaining posterior distribution g marginal likelihood corresponding graphical gaussian model method gives way sampling posterior distribution precision matrix criteria identifiability path analysis models one hidden variable first derive sufficient criteria identification models marginalisation carried hidden variable sufficient criteria based structure directed acyclic graph associated path analysis model derived graph treat identification models hidden variable conditioned establish connections extended skew normal distribution derived conditions extend existing graphical criteria identification focuses akaike information criterion aic linear mixed effects models analysis clustered data distinction questions regarding population questions regarding particular clusters data aic current appropriate focus clusters propose instead conditional akaike information corresponding criterion conditional aic caic penalty term caic related effective degrees freedom p linear mixed model proposed hodges sargent p reflects intermediate level complexity fixed effects model cluster effect corresponding model fixed cluster effects caic defined maximum likelihood residual maximum likelihood estimation pharmacokinetics data application used illuminate distinction two inference settings illustrate conditional aic model selection propose general dimension reduction method combines ideas likelihood correlation inverse regression information theory require dependence confined particular conditional moments place restrictions predictors regression necessary methods like ordinary least squares sliced inverse regression although focus single index regressions underlying idea applicable generally illustrative examples presented nordstrom robinson code well known nonlinear code coding theory explores statistical properties nonlinear code many nonregular designs runs factors derived nonregular designs better regular designs size terms resolution aberration projectivity furthermore many nonregular designs generalised minimum aberration among possible designs seven orthogonal arrays unique word length pattern four unique isomorphism consider problem maximum likelihood estimation case control gene environment associations disease genetic environmental exposures assumed independent underlying population traditional logistic regression analysis may efficient setting semiparametric maximum likelihood estimates logistic regression parameters exploit gene environment independence assumption leave distribution environmental exposures nonparametric profile likelihood technique derive simple algorithm obtaining estimator asymptotic theory extended situations genetic environmental factors independent conditional factors simulation investigate small sample properties method illustrated data case control designed investigate interplay brca mutations oral contraceptive aetiology ovarian cancer many applications functional data analysis summarising functional variation based fits without taking account estimation process runs risk attributing estimation variation functional variation thereby overstating latter example first eigenvalue sample covariance matrix computed estimated functions may biased upwards display set estimated neuronal poisson process intensity functions bias substantial discuss two methods accounting estimation variation one method uses random coefficient model requires functions fitted basis functions alternative method removes basis restriction means hierarchical gaussian process model small simulation hierarchical gaussian process model outperformed random coefficient model greatly reduced bias estimated first eigenvalue would result ignoring estimation variability neuronal data hierarchical gaussian process estimate first eigenvalue much smaller naive estimate ignored variability due function estimation neuronal setting illustrates benefit incorporating alignment parameters hierarchical scheme methods analysing cluster correlated biological data implicitly assume ignorability cluster sizes assumption fails resulting inferences may asymptotically invalid hoffman et al proposed simple computationally intensive method based large number within cluster resamples associated separate estimating equations leads asymptotically valid inferences whether cluster sizes ignorable simple method based single inverse cluster size weighted estimating equation avoids resampling yet leads asymptotically valid inferences simulation presented assess performance proposed method propose wald tests ignorability cluster sizes derive general formulae suitable monte carlo computation conditional expectations functions random vector given sufficient statistic problem direct sampling conditional distribution considered particular done simple parameter adjustment original statistical model provided model certain pivotal structure connection classical problem regarding fiducial posterior distributions pointed fisher bingham distribution obtained multivariate normal random vector conditioned unit length normalising constant expressed elementary function multiplied density evaluated linear combination independent noncentral chi random variables hence may approximate normalising constant applying saddlepoint approximation density three approximations implementation straightforward investigated first order saddlepoint density approximation second order saddlepoint density approximation variant second order approximation proved slightly accurate two numerical theoretical present approach highly accurate approximations broad spectrum cases weighted least absolute deviations estimator studied ar process arch errors c unlike quasi maximum likelihood estimator estimator limiting distribution normal even e epsilon infinity furthermore estimator applied examine symmetry density epsilon estimate quantity e log vertical bar alpha lambda epsilon vertical bar crucial importance conducting asymptotic inference quasi maximum likelihood estimators weighted least absolute deviations estimators generalised minimum aberration recently established design criterion whole class orthogonal arrays fractional factorial designs criterion name suggests generalisation minimum aberration regular designs minimum g aberration two level designs aim criterion designs minimise certain sense aliasing main effects interactions theoretical developed finding symmetrical orthogonal arrays generalised minimum aberration two factor levels empirical likelihood method derive test thus confidence interval based rank estimators regression coefficient accelerated failure time model standard chi squared distributions used calculate p value construct confidence interval simulations examples chi squared approximation distribution log empirical likelihood ratio performs well advantages existing methods consider general class empirical discrepancy includes cressie read discrepancy particular empirical likelihood ratio statistic order asymptotics expected lengths associated confidence intervals investigated explicit formula worked comparative purposes discussed seen empirical likelihood ratio statistic enjoys interesting second order power properties loses much edge present criterion standard approaches semiparametric modelling two sample survival data appropriate two survival curves cross introduce two sample model accommodates crossing survival curves two scalar parameters model interpretations short term long term hazard ratios respectively time varying hazard ratio expressed semiparametrically two scalar parameters unspecified baseline distribution new model includes cox model proportional odds model submodels inference pseudo maximum likelihood approach expressed via simple estimating equations analogous maximum partial likelihood estimator cox model consistent asymptotically normal estimators simulation estimators perform well moderate sample sizes illustrate methods real data example new model extended easily regression setting function time mean residual life remaining life expectancy given survival proportional mean residual life model proposed oakes dasu alternative cox proportional hazards model studying association survival times covariates presence censoring counting process theory develop semiparametric inference procedures regression coefficients oakes dasu model simulation application well known veterans administration lung cancer survival data presented empirical likelihood exhibit many properties conventional parametric likelihoods formal probabilistic interpretation far lacking likelihood function closely related empirical likelihood naturally arises nonparametric bayesian procedure places type noninformative prior space distributions prior gives preference distributions small support among sharing support favours entropy maximising distributions resulting nonparametric bayesian procedure admits computationally convenient representation empirical likelihood type likelihood probability weights obtained via exponential tilting proposed methodology attractive alternative bayesian bootstrap nonparametric limit bayesian procedure moment condition models probability matching priors priors posterior probabilities certain specified sets exactly approximately equal coverage probabilities priors arise solutions partial differential equations may difficult solve either analytically numerically recently levine casella presented algorithm implementation probability matching priors interest parameter presence single nuisance parameter develop local implementation much easily computed local probability matching prior data dependent approximation probability matching prior asymptotic order approximation frequentist coverage probability degraded illustrate theory number examples including three discussed levine casella proposes classical weighted least squares type local polynomial smoothing analysis clustered data key idea generalised inverses correlation matrices estimator simple closed form expression simplicity achieved nonparametric generalised linear models arbitrary link function via transformation approach characterised local observations local variances yields intuitively correct sense correct incorrect specification within cluster correlation respective positive negative effects approach natural extension classical local polynomial smoothing consequently existing theory largely carried issues bandwidth selection tackled classical fashion moreover approach handle various types covariate cluster level level partially cluster level numerical support theoretical method illustrated real example luteinising hormone levels cows introduce covariate adjusted regression situations predictors response regression model directly observable contaminated multiplicative factor determined value unknown function observable covariate regression coefficients estimated establishing connection varying coefficient regression proposed covariate adjustment method illustrated analysis regression plasma fibrinogen concentration response serum transferrin level predictor haemodialysis patients example response predictor thought influenced multiplicative fashion body mass index bootstrap test enables us test significance regression parameters establish consistency convergence rates parameter estimators new covariate adjusted regression model simulation efficacy proposed method penalised spline based additive models allow simple mixed model representation variance components control departures linear models smoothing parameter ratio random coefficient error variances tests linear regression reduce tests zero random coefficient variances propose exact likelihood restricted likelihood ratio tests testing polynomial regression versus general alternative modelled penalised splines spectral decompositions used basis fast simulation algorithms derive asymptotic local power properties tests weak conditions particular characterise local alternatives detected asymptotic probability one confidence intervals smoothing parameter obtained inverting tests fixed smoothing parameter versus general alternative discuss f r tests ignoring variability smoothing parameter estimator dramatic effect null distributions powers several known tests investigated small set tests good power properties identified restricted likelihood ratio test among best terms power penalised spline regression popular new approach smoothing theoretical properties yet well understood mean squared error expressions consistency derived white noise model representation estimator effect penalty bias variance estimator discussed general splines case polynomial splines penalised spline regression estimator achieve optimal nonparametric convergence rate established stone present method extracting information scale trend local components inhomogeneous function nonparametric generalised linear model multiscale framework combines recursive partitions allow incorporation scale natural manner systems piecewise polynomials supported partition intervals serve summarise smooth trend within interval estimators formulated solutions complexity penalised likelihood optimisations penalty seeks limit number intervals used model data actual calculation estimators may accomplished standard software routines generalised linear models within context efficient tree based polynomial time algorithms risk analysis estimators achieve asymptotic rates nonparametric generalised linear model classical wavelet based estimators gaussian function plus noise model suitably defined ranges besov spaces numerical simulations method tends perform least well often better alternative wavelet based methodologies context finite samples applications gamma ray burst data astronomy packet loss data computer network traffic analysis confirm practical relevance fourier series used basis inference deconvolution problems effects errors factorise way easily exploited empirically property consequence elementary addition formulae sine cosine functions readily available one methods based orthogonal series continuous fourier transforms allows relatively simple estimators constructed founded addition finite series rather integration performance methods particularly effective edge effects involved since cosine series estimators quite resistant boundary problems context point advantages trigonometric series methods density deconvolution better mean squared error performance edge effects involved particularly easy code admit simple approach empirical choice smoothing parameter version thresholding familiar wavelet based inference used place conventional smoothing applications deconvolution problems briefly discussed pseudo bayesian interpretation standard errors yields natural induced smoothing statistical estimating functions applied rank estimation lack smoothness prevents standard error estimation remedied efficiency robustness preserved smoothed estimation excellent computational properties particular convergence iterative equation standard error fast standard error calculation becomes asymptotically one step procedure property extends covariance matrix calculation rank estimates multi parameter problems examples simple explanations given derive nonparametric procedures testing likelihood ratio ordering two population setting continuous distributions account ordering examining least concave majorant ordinal dominance curve formed nonparametric maximum likelihood estimators continuous distribution functions f g particular focus testing equality f g versus likelihood ratio ordering testing violation likelihood ratio ordering testing problems propose area based sup norm based test derive appropriate limiting distributions simulation characterise performance procedures illustrate methods data controlled experiment involving effects radiation mice asymptotic multivariate normal approximations joint distributions edge exclusion test saturated graphical gaussian models derived non signed signed square root versions likelihood ratio wald score test considered noncentral chi squared approximations considered non signed versions approximations used estimate power edge exclusion tests example presented based basic area level model obtain second order accurate approximations mean squared error model based small area estimators fay herriot iterative method estimating model variance based weighted residual sum squares obtain mean squared error estimators unbiased second order based simulations compare finite sample performance mean squared error estimators based method moments maximum likelihood residual maximum likelihood estimators model variance suggest fay herriot method performs better terms relative bias mean squared error estimators methods across different combinations number areas pattern sampling variances distribution small area effects derive noninformative prior model parameters posterior variance small area mean second order unbiased mean squared error posterior variance based prior possesses bayesian frequentist interpretations main objective clinical trials best treatment given finite class competing treatments superiority treatment control treatment traditional procedure estimates best treatment first trial independent second trial superiority treatment estimated best first trial control treatment size alpha test investigate two trials traditional procedure two stage test procedure additionally introduce competing two stage group sequential test procedures derive formulae expected number patients formulae depend unknown parameters prior unknown parameters determine two stage test procedure size power beta optimal needs minimal number observations illustrated numerical example indicates superiority group sequential procedures multivariate extreme value distributions arise limiting distributions normalised componentwise maxima often used model multivariate data regarded componentwise maxima unobserved underlying multivariate process many applications extra information often know locations maxima within underlying process process temporal knowledge frequently available dates maxima recorded incorporate extra information maximum likelihood procedures asymptotic small sample efficiency presented dependence parameter logistic parametric sub class bivariate extreme value distributions conclude application sea levels part folklore capture recapture experiments ignoring heterogeneity capture probabilities downward bias based experience simulation often interpreted due individuals capture probabilities estimating equation arguments used effect horvitz thompson type estimators ignoring heterogeneity capture recapture experiments introduce downward bias arguments extended continuous time experiments influence function constructed determine effect small number individuals heterogeneous capture probabilities otherwise homogeneous population influence function negative downward bias holds even small number heterogeneous individuals capture probabilities larger homogeneous majority confirmed simulations beran b hall simple linear interpolation convenient approach constructing nonparametric confidence intervals population quantiles based random sample size n coverage error interpolated interval order o n improved upon calibrating nominal coverage level three distinct methods calibration considered analytical monte carlo methods succeed reducing order coverage error o n smoothed bootstrap method reduces o n guidelines practical implementation calibration methods performance compared simple linear interpolated interval simulation confirms superiority calibrated intervals employ lasso shrinkage within context sufficient dimension reduction obtain shrinkage sliced inverse regression estimator easier interpretations better prediction accuracy without assuming parametric model shrinkage sliced inverse regression approach employed single index multiple index models simulation suggest new estimator performs well tuning parameter selected either bayesian information criterion residual information criterion
