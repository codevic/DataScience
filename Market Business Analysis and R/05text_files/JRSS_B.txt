problems analysis data incomplete observations familiar doubly difficult uncertain choice model propose general formulation discussion problems develop approximations resulting bias maximum likelihood estimates assumption model departures small loss efficiency parameter estimation due incompleteness data dual interpretation increase variance assumed model correct bias estimation model incorrect examples include non ignorable missing data hidden confounders observational publication bias meta analysis doubling variances calculating confidence intervals test suggested crude way addressing possibility undetectably small departures model problem assessing risk lung cancer passive smoking used motivating example introduce flexible marginal modelling approach statistical inference clustered longitudinal data minimal assumptions estimated estimating equations approach semiparametric proposed models fitted quasi likelihood regression unknown marginal means function fixed effects linear predictor unknown smooth link variance covariance unknown smooth function marginal means propose estimate nonparametric link variance covariance functions via smoothing methods whereas regression parameters obtained via estimated estimating equations score equations contain nonparametric function estimates proposed estimated estimating equations approach motivated flexibility easy implementation moreover data follow generalized linear mixed model either specified unspecified distribution random effects link function model proposed emerges corresponding marginal population average version used obtain inference fixed effects underlying generalized linear mixed model without need specify components generalized linear mixed model among marginal models estimated estimating equations approach flexible alternative modelling generalized estimating equations applications estimated estimating equations include diagnostics link selection asymptotic distribution proposed estimators model parameters derived enabling statistical inference practical illustrations include poisson modelling repeated epileptic seizure counts simulations clustered binomial responses consider problem estimating proportion true null hypotheses pi multiple set tests based observed p values first published estimators based estimator suggested schweder spjotvoll derive new estimators based nonparametric maximum likelihood estimation p value density restricting decreasing convex decreasing densities estimators pi derived assumption independent test performance dependence investigated simulation estimators relatively robust respect assumption independence work well test moderate dependence graph theoretical approach employed describe support set nonparametric maximum likelihood estimator cumulative distribution function given interval censored left truncated data necessary sufficient condition existence nonparametric maximum likelihood estimator derived two previously analysed data sets revisited recently much work developing models suitable analysing volatility continuous time process one general approach define volatility process convolution kernel non decreasing levy process non negative kernel non negative within framework time continuous autoregressive moving average carma processes derive necessary sufficient condition kernel non negative condition terms laplace transform carma kernel simple form discuss useful consequences result delineate parametric region stationarity non negative kernel order carma models introduce directionally dispersed class multivariate distributions generalization elliptical class allowing dispersion multivariate random variables vary direction possible generate wide flexible class distributions directionally dispersed distributions simple form density extends spherically symmetric density function including function d modelling directional dispersion mild condition class distributions preserve unimodality moment existence adequately defining d possible generate skewed distributions spline models hyperspheres suggest flexible yet practical implementation modelling directional dispersion dimension new class distributions bayesian regression set analyse distributions set biomedical measurements sample us manufacturing firms concerned problem finding optimum experimental design discriminating two rival multiresponse models criterion optimality based sum squares deviations models picks design points divergence maximum part criterion additional vector experimental conditions may affect design give necessary conditions design additional parameters experiment optimum present algorithm numerical optimization procedure relevance methods dynamic systems especially chemical kinetic models consider problem estimating noise variance homoscedastic nonparametric regression models dimensional covariates element r d d difference based estimators investigated series papers given length estimator difference schemes minimize asymptotic mean squared error computed d d numerical known finite sample sizes performance estimators may deficient owing large finite sample bias theoretical support particular increasing dimension d becomes drastic dgreater equal estimators even fail consistent different class estimators discussed allow better control bias remain consistent dgreater equal estimators compared numerically kernel type estimators asymptotically efficient guidance given becomes necessary compared classical backfitting buja hastie tibshirani smooth backfitting estimator sbe mammen linton nielsen complete asymptotic theory weaker conditions efficient robust easier calculate original describing sbe method complex practical well theoretical advantages method still neither recognized accepted statistical community focus clear presentation idea main theoretical practical aspects like implementation simplification algorithm introduce feasible cross validation procedure apply problem data driven bandwidth choice sbe simulations sbe cross validation work well indeed particular sbe less affected sparseness data dimensional regression problems strongly correlated designs sbe reasonable performance even dimensional additive regression problems multivariate linear regression includes multivariate normal models models used survival analysis variety models used areas econometrics considers class location scale models includes large proportion preceding models complete data maximum likelihood estimators regression coefficients linear location scale framework consistent even joint distribution misspecified addition gains efficiency arising bivariate model opposed separate univariate models studied major area application multivariate regression models clustered parallel lifetime data case censored responses estimators regression coefficients longer consistent model misspecification give simulation bias small many practical situations gains efficiency bivariate models examined censored data setting methodology illustrated lifetime data diabetic retinopathy jackknife method often used variance estimation sample surveys developed limited class sampling designs propose jackknife variance estimator defined without replacement unequal probability sampling design design consistency estimator broad class point estimators monte carlo proposed estimator may improve existing estimators lasso penalizes least squares regression sum absolute values l norm coefficients form penalty encourages sparse solutions many coefficients equal propose fused lasso generalization designed problems features ordered meaningful way fused lasso penalizes l norm coefficients successive differences thus encourages sparsity coefficients sparsity differences e local constancy coefficient profile fused lasso especially useful number features p much greater n sample size technique extended hinge loss function underlies support vector classifier illustrate methods examples protein mass spectroscopy gene expression data describe quantum tomography inverse statistical problem quantum state light beam unknown parameter data given measurements performed identical quantum systems state represented infinite dimensional density matrix equivalently density plane called wigner function present consistency pattern function projection estimators sieve maximum likelihood estimators density matrix quantum state wigner function illustrate performance estimators simulated data em algorithm proposed practical implementation remain many open problems e g rates convergence adaptation studying estimators main purpose bring attention statistical community considers modelling estimating diagnostically verifying response process generating longitudinal data emphasis association repeated meas ures unbalanced longitudinal designs model based separate specifications moments mean standard deviation correlation different components possibly sharing common parameters propose general class correlation structures comprise random effects measurement errors serially correlated process three elements combined via flexible time varying weights whereas serial correlation depend flexibly mean time lag measurement schedule independent response process estimation procedure yields consistent asymptotically normal estimates mean parameters even standard deviation correlation misspecified standard deviation parameters even correlation misspecified generic diagnostic method developed verifying models mean standard deviation particular correlation applicable even data severely unbalanced methodology illustrated analysis data longitudinal designed characterize pulmonary growth girls usual design unbiased estimators adaptive cluster sampling easy compute functions minimal sufficient statistic hence improved improved unbiased estimators obtained conditioning sufficient necessarily minimal described first estimators easy compute usual design unbiased estimators given estimators obtained conditioning minimal sufficient statistic difficult compute discussed estimators compared examples longitudinal missingness data often unavoidable problem estimators linear mixed effects model assume missing data missing random estimators biased assumption met theoretical asymptotic bias established non ignorable drop drop missing data patterns asymptotic bias large drop one observation especially slope related parameters linear mixed effects model drop case intercept related parameter estimators substantial asymptotic bias enter late eight missing data patterns considered produce asymptotic biases variety magnitudes empirical bayes techniques normal theory shrinkage estimation extended generalized linear models manner retaining original spirit shrinkage estimation reduce risk investigation identifies two classes simple purpose prior distributions supplement non informative priors jeffreys prior mechanisms risk reduction one new class priors motivated optimizers core component asymptotic risk methodology evaluated numerical exploration application existing data set motivated statistical inference problem population genetics present new sequential importance sampling resampling strategy idea resampling key recent surge popularity sequential monte carlo methods engin eering communities existing resampling techniques work well coalescent based inference problems population genetics develop new method called stopping time resampling allows us compare partially simulated samples different stages terminate unpromising partial samples multiply promising samples early illustrate idea first apply new method approximate solution dirichlet problem likelihood function non markovian process focus application population genetics examples new resampling method significantly improve computational efficiency existing sequential importance sampling methods precise classification tumours critical diagnosis treatment cancer diagnostic pathology traditionally relied macroscopic microscopic histology tumour morphology basis classification tumours current classification frameworks cannot discriminate tumours similar histopathologic features vary clinical course response treatment recent move towards complementary deoxyribonucleic acid microarrays classi fication tumours throughput assays relative messenger ribonucleic acid expression measurements simultaneously thousands genes key statistical task perform classification via different expression patterns gene expression profiles may offer information classical morphology may alternative classical tumour diagnosis schemes considers several bayesian classification methods based reproducing kernel hilbert spaces analysis microarray data consider logistic likelihood well likelihoods related support vector machine models simulation examples support vector machine models multiple shrinkage parameters produce fewer misclassification errors several existing classical methods well bayesian methods based logistic likelihood involving one shrinkage parameter expectation maximization em algorithm popular tool maximizing likelihood functions presence missing data unfortunately em often requires evaluation analytically intractable dimensional integrals monte carlo em mcem algorithm natural extension em employs monte carlo methods estimate relevant integrals typically large monte carlo sample size required estimate integrals within acceptable tolerance algorithm near convergence even sample size known onset implementation mcem throughout iterations wasteful especially accurate starting values available propose data driven strategy controlling monte carlo resources mcem algorithm proposed improves similar existing methods recovering em ascent e likelihood increasing property probability robust effect defined inputs handling classical monte carlo markov chain monte carlo methods within common framework first properties refer algorithm ascent based mcem apply ascent based mcem variety examples including one used accelerate convergence deterministic em dramatically considers dimensional metropolis langevin algorithms initial transient phase stationarity algorithms well understood well known scale proposal distribution variances random walk metropolis algorithm convergence transient phase extremely regular extent algo rithm sample path actually resembles deterministic trajectory contrast langevin algorithm variance scaled optimal stationarity performs rather erratically give weak convergence explain types behaviour practical guidance implementation based theory deal contingency table data used examine relationships set categorical variables factors assume relationships adequately described cond itional independence structure imposed undirected graphical model contingency table large desirable simplified interpretation achieved combining categories levels factors introduce conditions operation alter markov properties graph implementation conditions leads bayesian model uncertainty procedures based reversible jump markov chain monte carlo methods methodology illustrated x x x x x x contingency table importance variable selection regression grown recent computing power encouraged modelling data sets ever increasing size data mining applications finance marketing bioinformatics obvious examples limitation nearly existing variable selection methods need specify correct model selection number predictors large model formulation validation difficult even infeasible basis theory sufficient dimension reduction propose new class model free variable selection approaches methods proposed assume model form require nonparametric smoothing allow general predictor effects efficacy methods proposed demonstrated via simulation empirical example given propose elastic net new regularization variable selection method real world data simulation elastic net often outperforms lasso enjoying similar sparsity representation addition elastic net encourages grouping effect strongly correlated predictors tend model together elastic net particularly useful number predictors p much bigger number observations n contrast lasso satisfactory variable selection method p n case algorithm called lars en proposed computing elastic net regularization paths efficiently much like algorithm lars lasso modelling multivariate financial data problem structural learning compounded fact covariance structure changes time previous work focused modelling changes multivariate stochastic volatility models present alternative models focuses instead latent graphical structure related precision matrix develop graphical model sequences gaussian random vectors changes underlying graph occur random times new block data created addition deletion edge bayesian hierarchical model incorporates uncertainty graph time variation thereof suppose x k variate spherically symmetric distribution mean vector theta identity covariance matrix present two spherical confidence sets theta centred positive part stein estimator x first obtain radius approximating upper alpha point sampling distribution parallel x theta parallel first two non zero terms taylor series origin analyse properties confidence set see performs well terms coverage probability volume conditional behaviour second method radius parametric bootstrap procedure even greater improvement terms volume usual confidence set possible expense less explicit radius function real data example provided extensions unknown covariance matrix elliptically symmetric cases discussed bagging computationally intensive method asymptotically improves performance nearest neighbour classifiers provided resample size less actual sample size case replacement bagging less sample size without replacement bagging larger sampling fractions asymptotic difference risk regular nearest neighbour classifier bagged version particular neither achieves large sample performance bayes classifier contrast sampling fractions converge resample sizes diverge infinity bagged classifier converges optimal bayes rule risk converges risk latter readily seen two populations well defined densities may derived cases densities exist relative sense cross validation used effectively choose sampling fraction numerical calculation used illustrate theoretical properties undertake statistical inference infinite variance autoregressive models long standing open problem solve problem propose self weighted least absolute deviation estimator estimator asymptotically normal density errors derivative uniformly bounded furthermore wald test statistic developed linear restriction parameters non trivial local power simulation experiments carried assess performance theory method finite samples real data example given entirely different published new insights future research heavy tailed time series likelihood inference discretely observed markov jump processes finite state space investigated existence uniqueness maximum likelihood estimator intensity matrix investigated topic closely related imbedding problem markov chains demonstrated maximum likelihood estimator found either em algorithm markov chain monte carlo procedure maximum likelihood estimator exist estimator obtained penalized likelihood function markov chain monte carlo procedure suitable prior methodology implementation illustrated examples simulation throughput genomic work large number d hypotheses tested based n d data samples large number tests necessitates adjustment false discoveries true null rejected expected number false discoveries easy obtain dependences tests greatly affect variance number false discoveries assuming tests independent gives inadequate variance formula presents variance formula takes account correlations test formula involves o d correlations naive implementation cost o nd method based sampling pairs tests allows variance approximated cost independent d dimension sample size data emerging various areas science common structure underlying many data sets non standard type asymptotics dimension tends infinity sample size fixed analysis tendency data lie deterministically vertices regular simplex essentially randomness data appears random rotation simplex geometric representation used obtain several new statistical insights propose calibrated imputation compensate missing values technique consists finding final imputed values close possible preliminary imputed values calibrated satisfy constraints preliminary imputed values potentially justified imputation model obtained deterministic single imputation appropriate constraints resulting imputed estimator asymptotically unbiased estimation linear population parameters domain totals quasi model assisted approach considered sense inferences depend validity imputation model made respect sampling design non response model imputation model may still used generate imputed values thus improve efficiency imputed estimator approach characteristic handling naturally situation one imputation method used owing missing values variables used obtain imputed values taylor linearization technique obtain variance estimator general non response model logistic non response model ignoring effect estimating non response model parameters leads overestimating variance imputed estimator practice overestimation expected moderate even negligible simulation define residuals point process models fitted spatial point pattern data propose diagnostic plots based residuals apply point process model conditional intensity model may exhibit spatial heterogeneity interpoint interaction dependence spatial covariates existing ad hoc methods model checking quadrat counts scan statistic kernel smoothed intensity berman diagnostic recovered special cases diagnostic tools developed systematically analogy spatial residuals usual residuals non spatial generalized linear models conditional intensity lambda plays role mean response makes possible adapt existing knowledge model validation generalized linear models spatial point process context giving recommendations diagnostic plots plot smoothed residuals spatial location spatial covariate effective diagnosing spatial trend co variate effects q q plots residuals effective diagnosing interpoint interaction meteorological environmental data collected regular time intervals fixed monitoring network usefully studied combining ideas multiple time series spatial particularly little missing data work investigates methods modelling data ways approximating associated likelihood functions models processes sphere crossed time emphasized especially models fully symmetric space time two approaches obtaining models described first consider rotated version fully symmetric models explicit expressions covariance function second based representation space time covariance functions spectral time domain lead natural partially nonparametric asymmetric models sphere crossed time various models applied data set daily winds sites ireland spectral space time domain diagnostic procedures used assess quality fits spectral time modelling approach yield good fit many properties data applied routine fashion relative finding elaborate parametric models describe space time dependences data well traditionally bayes factors required specification proper prior distributions model parameters implicit null alternative hypotheses describe approach defining bayes factors based modelling test distributions test depend unknown model parameters approach eliminates much subjectivity normally associated definition bayes factors standard test including chi f z values bayes factors result approach simple closed form expressions develop new class time continuous autoregressive fractionally integrated moving average carfima models useful modelling regularly spaced irregu larly spaced discrete time long memory data derive autocovariance function stationary carfima model maximum likelihood estimation regression model carfima errors based discrete time data via innovations algorithm maximum likelihood estimator asymptotically normal finite sample properties studied simulation efficacy approach proposed demonstrated data set environmental consider maximum likelihood methods estimating end point distribution likelihood function modified prior distribution imposed location parameter prior explicit meaningful general form adapts different settings convergence rates limiting distributions given particular limiting distribution non normal non regular cases parametric bootstrap techniques suggested quantifying accuracy estimator illustrate performance applying method multiparameter weibull gamma distributions concerned new methodology statistical inference final outcome infectious disease data certain structured population stochastic epidemic models major obstacle inference models likelihood analytically numerically intractable approach taken impute missing information form random graph describes potential infectious contacts individuals level imputation overcomes various constraints existing methodologies yields detailed information spread disease methods illustrated real test data building game theoretic framework probability possible randomization sequential probability forecasts pass given battery statistical tests result easy consequence von neumann minimax theorem simplifies generalizes work earlier researchers
