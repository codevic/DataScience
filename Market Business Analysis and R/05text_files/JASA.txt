statistical agencies release microdata public malicious users intruders may able link records released data records external databases releasing data ways fail prevent identifications may discredit agency data constitute breach law limit disclosures agencies often release altered versions data usually remain risks identification applies extends framework developed duncan lambert computing probabilities identification sampled units describes methods tailored specifically data altered recoding topcoding variables data swapping adding random noise combinations common data alteration techniques agencies assess threats intruders possess information relationships among variables methods data alteration data current population survey illustrates step step process evaluating identification disclosure risks competing releases varying assumptions intruders knowledge risk measures presented individual units entire datasets best decades electoral polls striking errors occurring among others british french spanish elections including election night errors evident proposes model predicting final election outcomes based consistency polling stations elections past incoming polling station vote proportions model produces continuously revised predictions method validated predicting corts valencianes valencia regional parliament elections displaying real time experience corts valencianes election night case completed demonstrating technique efficacy three additional elections confirm procedure generates quick highly reliable accurate forecasts fact minutes starting scrutiny proposal permits one approximate final great precision even small percentage votes polled great flexibility procedure makes possible method wide variety circumstances electoral systems furthermore procedure additional advantages including robustness cost methods implemented election night objective forecasting final outcomes like exit polls quick counts meaningful sample polling stations administrative systems specifically cancer registries valuable data source health care provision adjuvant chemotherapy radiation therapy often underreported databases colorectal cancer california relatively small physician follow back survey allowed us model probability underreporting wished model relationship true treatment status covariates full database developed hierarchical models imputation corrected data data recorded error administrative system validation sample survey model includes model probability receipt chemotherapy model probability reporting given chemotherapy factorization joint distribution true status reported data designed permit generalization validation sample larger population reporting process similar prevalence treatment may differ hospital random effects included represent variation treatment reporting patterns across hospitals used markov chain monte carlo simulation techniques estimate model parameters impute true treatment status valid inferences obtained combining multiply imputed datasets analysis predictors survival imputed data corrected bias due underreporting uncertainty due underreporting chemotherapy substantially inflated variance estimates chemotherapy effect little effect estimation coefficients characteristics good short period forecast heavy rainfall essential many meteorological hydrological applications traditional deterministic stochastic nowcasting methodologies inadequate characterization pixelwise rainfall reflectivity propagation intensity uncertainty methodology presented herein uses approach efficiently parameterizes spatio temporal dynamic models terms integro difference equations within hierarchical framework approach accounts uncertainty prediction relevant distributional information concerning nowcast application presented effectiveness technique potential nowcasting weather radar reflectivities presents examines new algorithm solving score equation maximum likelihood estimate certain problems practical interest method circumvents need compute second order derivatives full likelihood function exploits structure certain models yield natural decomposition complicated likelihood function decomposition first part log likelihood simply analyzed model second part used update estimates first part convergence properties iterative fixed point algorithm examined asymptotics derived estimators obtained finite number iterations illustrative examples considered include multivariate gaussian copula models nonnormal random effects models generalized linear mixed models state space models properties algorithm estimators evaluated simulation bivariate copula model nonnormal linear random effects model inequality provided determines shrinkage reduces mean squared error mse unbiased estimate artificially augmented samples used obtain among others shrinkage estimates population variance covariance improve unbiased estimates parameter values probability models marginals finite second moments alternative jackknife estimates complement usual jackknife estimates reducing mse consider robust generalized estimating equations analysis semiparametric generalized partial linear models gplms longitudinal data clustered data general approximate nonparametric function gplm regression spline bounded scores leverage based weights estimating equation achieve robustness outliers regression spline approach avoids intricacies associated profile kernel method robust estimation inference carried operationally generalized linear model used addresses problem finding relationship univariate predictor response regression errors created part known auxiliary covariates large reliable regression estimation typical example controlled random design experiment large number covariates statistician interested effect particular covariate effect blurred large regression noise created covariates develops theory asymptotically optimal nonparametric univariate regression estimation presence auxiliary covariates optimality means mimicking performance oracle knows effects auxiliary covariates response asymptotic theory optimal estimation possible explains evaluate noise created auxiliary covariates develop estimator interesting case small sample sizes concept modeling regression noise well known analysis covariance ancova applied optimal way nonparametric regression setting procedure small sample sizes denoised scattergram tested simulated examples real dataset observations auxiliary covariates justify practical feasibility developed method method allows practitioner visualize dataset would appear effects auxiliary covariates eliminated determine exhibited regression function given particular shape many practical recommendations particular known shape restrictions presented discussed asymptotic theory numerical analysis real dataset indicate proposed method reducing variance regression errors created auxiliary covariances feasible easy implement improves likelihood meaningful regression analysis model combining e mixing methods proposed recent deal uncertainty model selection even though advantages model combining model selection demonstrated simulations data examples still unclear large extent model combining preferred work first propose instability measure capture uncertainty model selection estimation called perturbation instability estimation pie based perturbation sample estimators model selection large pie values model combining substantially reduces instability cases second propose model combining method adaptive regression mixing model screening arms derive theoretical property arms screening step taken narrow list candidate models combining saves computing time improve estimation accuracy third compare arms ebma empirical bayesian model averaging model selection methods number simulations real data examples comparison model combining produces better estimators instability model selection arms performs better ebma cases simulations respect choice model selection model combining propose rule thumb terms pie empirical support pie sensible indicator model selection instability estimation useful understanding whether model combining better choice model selection data hand propose empirical bayes method variable selection coefficient estimation linear regression models method based particular hierarchical bayes formulation empirical bayes estimator closely related lasso estimator connection allows us take advantage recently developed quick lasso algorithm compute empirical bayes estimate new way select tuning parameter lasso method unlike previous empirical bayes variable selection methods practical situations implemented greedy stepwise algorithm method gives global solution efficiently simulations real examples proposed method competitive terms variable selection estimation accuracy computation speed compared variable selection estimation methods estimation conditional quantiles counts given discreteness data smoothness must artificially imposed problem possible smooth data way allows inference performed standard quantile regression techniques performance implementation estimators illustrated simulations application concerned estimating additive components nonparametric additive quantile regression model develop estimator asymptotically normally distributed rate convergence probability n r r additive components r times continuously differentiable r result holds regardless dimension covariates thus new estimator curse dimensionality addition estimator oracle property easily extended generalized additive quantile regression model link function numerical performance usefulness estimator illustrated monte carlo experiments empirical example missing covariate data common epidemiologic disease prevention trials regression parameter estimation cox proportional hazards model considered certain covariates observed covariate data collected subset presents simple weighted kernel assisted fully augmented weighted estimators partially incomplete data nonparametrically nonparametric methods estimate selection probabilities simple weighted estimating functions nonparametric kernel smoothing techniques estimate certain conditional expectations fully augmented weighted estimating functions proposed methods nonparametric sense require neither model missing data mechanism specification conditional distribution missing covariates given observed covariates estimators allow missing data mechanism depend outcome variables observed covariates applicable various cohort sampling procedures including case cohort nested case control designs simple kernel assisted fully augmented weighted estimators typically consistent asymptotically normal moreover proposed estimators efficient simple weighted estimator inverse true selection probability weight correct bias estimates analysis complete data alone missing data mechanism depends outcome variables addition covariates time independent certain simple weighted estimators asymptotically equivalent kernel assisted fully augmented weighted estimators moderate sample size performance estimators examined via simulation application two real datasets presents estimator regression coefficient vector cox proportional hazards model covariate error estimator obtained maximizing likelihood type function similar cox partial likelihood likelihood function involves cumulative baseline hazard function simple estimator substituted method capable handling general covariate error structures restricted independent additive error model applied either external internal validation sample replicate measurements surrogate covariate estimator consistent asymptotically normal estimate asymptotic covariance matrix derived extensions general transformation survival models indicated simulation presented setup single error prone binary covariate setup single error prone normally distributed covariate simulation method typically produces estimates bias confidence intervals accurate coverage rates efficiency relative fully parametric maximum likelihood presented method applied data framingham heart recent dirichlet process prior experienced great success context bayesian mixture modeling idea overcoming discreteness realizations exploiting hierarchical models combined development suitable sampling techniques represent one reasons popularity propose normalized inverse gaussian n ig process alternative dirichlet process used bayesian hierarchical models n ig prior constructed via finite dimensional distributions prior although sharing discreteness property dirichlet prior characterized elaborate sensible clustering makes information contained data whereas dirichlet case mass assigned observation depends solely number times occurred n ig prior weight single observation depends heavily whole number ties sample moreover expressions corresponding relevant statistical quantities priori moments predictive distributions tractable arising dirichlet process implies well established sampling schemes easily extended cover hierarchical models based n ig process mixture n ig process mixture dirichlet process compared two examples involving mixtures normals past decade seen remarkable development area bayesian nonparametric inference theoretical applied perspectives latter celebrated dirichlet process successfully exploited within bayesian mixture models leading many interesting applications former new discrete nonparametric priors recently proposed natural alternatives dirichlet process bayesian hierarchical model density estimation models concrete applications investigation statistical properties mandatory properties prominent role assigned consistency indeed strong consistency bayesian nonparametric procedures density estimation focus considerable amount research particular much attention devoted normal mixture dirichlet process improve previous contributions establishing strong consistency mixture dirichlet process fairly general conditions besides usual kullback leibler support condition consistency achieved finiteness mean base measure dirichlet process exponential decay prior standard deviation conditions sufficient mixtures based priors general dirichlet process leads easy establishment consistency many recently proposed mixture models propose new approach selection regression models based combining robust penalized criterion robust conditional expected prediction loss function estimated stratified bootstrap components procedure robust criteria e robust rho functions rather squared error loss reduce effects large residuals poor bootstrap samples key idea separate estimation model selection choosing estimators separately rho function stratified bootstrap reduces likelihood obtaining poor bootstrap samples model selection procedure consistent conditions works well simulations particular simultaneous minimization prediction error conditional expected prediction loss better separate minimization prediction error conditional expected prediction loss inference complex system rough energy landscape central topic monte carlo computation motivated successes wang landau algorithm discrete systems generalize algorithm continuous systems generalized algorithm features conventional monte carlo algorithms first new method monte carlo integration based stochastic approximation second excellent tool monte carlo optimization appropriate setting algorithm lead random walk energy space thus sample relevant parts sample space even presence many local energy minima generalized algorithm conveniently used many problems monte carlo integration optimization example normalizing constant estimation model selection highest posterior density interval construction function optimization numerical algorithm outperforms simulated annealing parallel tempering optimization system rough energy landscape theoretical convergence algorithm provided many fields science multivariate observations may assumed generated physical linear mixing process contributions different sources compositions sources constant different observations observations random error term nonnegative linear combinations fixed set called source profiles characterize sources goal linear unmixing recover source profiles source activities called scores multivariate dataset present new parametric mixing model assumes multivariate lognormal distribution scores model proved identifiable moreover consistency asymptotic normality maximum likelihood estimator mle established special cases calculate mle propose combination two variants monte carlo em algorithm proposed model applied simulated datasets set air pollution measurements addition basic model several extensions discussed considers class multiresolution tree structured models spatially shifted versions proposes new spatial prediction method averages optimal spatial predictors produced members class models consequence resulting predicted surface smooth even predictors generated separately individual multiresolution tree structured models call new predictor multiresolution spatial murs predictor develop computationally efficient algorithm algorithm handle massive datasets even observations missing moreover murs predictor minimum mean squared error predictor large class covariance functions simulation example massive datasets murs method consistently outperforms two commonly used filtering methods total column ozone data remotely sensed satellite analyzed new methodology consider problem image segmentation goal determine label relatively small number homogeneous subregions image scene based multivariate pixelwise measurements motivated current challenges field remote sensing land cover characterization introduce framework allows adaptive choice spatial resolution subregions categorical granularity labels framework based class models call mixlets blending recursive dyadic partitions finite mixture models first component models allows sparse representation spatial structure multiple resolutions second component natural mechanism capturing varying degrees mixing pure categories accompany different resolutions relating specified hierarchy labels multiple granularities straightforward manner segmentation produced framework selecting optimal mixlet model complexity penalized maximum likelihood summarizing information model respect categorical hierarchy theoretical empirical evaluations proposed framework presented construct efficient designs michaelis menten enzyme kinetic model capable checking model assumptions extended model called emax considered purpose model widely used pharmacokinetics reduces michaelis menten model specific choice parameter settings strategy efficient designs estimating parameters emax model time test validity michaelis menten model emax model maximizing minimum d d efficiencies taken range values nonlinear parameters particular designs efficient estimating parameters emax model b efficient estimating parameters michaelis menten model c efficient testing michaelis menten model emax model d robust respect misspecification unknown parameters nonlinear model heavy tailed distributions enjoying increased popularity becoming readily applicable arsenal analytical numerical tools grows play key roles modeling approaches networking finance hydrology name areas tail parameter alpha central importance governs existence moments positive order thickness tails distribution best known tail estimators koutrouvelis hill either parametric lack robustness accuracy develops shift scale invariant nonparametric estimator upper bounds orders finite moments estimator builds equivalence tail behavior regularity characteristic function origin achieves goal deriving simplified wavelet analysis particularly suited characteristic functions common practice finance estimate volatility sum frequently sampled squared returns market microstructure poses challenges estimation approach evidenced recent empirical finance present work attempts lay theoretical grounds reconcile continuous time modeling discrete time samples propose estimation approach takes advantage rich sources tick tick data preserving continuous time assumption underlying returns framework becomes clear usual volatility estimator fails returns sampled highest frequencies noise asymptotically small work way finding optimal sampling frequency better approach two scales estimator works size noise sequential monte carlo methods especially particle filter pf various modifications used effectively dealing stochastic dynamic systems standard pf samples current state underlying state dynamics uses current observation evaluate sample importance weight set problems current observation significant information current state state dynamics weak thus sampling current observation often produces efficient samples sampling state dynamics propose new variant pf independent particle filter ipf deal problems ipf generates exchangeable samples current state sampling distribution conditionally independent previous states special case uses current observation sample matched multiple samples previous states evaluating importance weight present theoretical showing strategy improves efficiency estimation well reduces resampling frequency discuss extensions ipf several synthetic examples effectiveness method propose new family symmetric unimodal distributions circle contains uniform von mises cardioid wrapped cauchy distributions among others special cases basic form densities family simple although normalization constant involves associated legendre function family distributions derived conditioning projecting certain bivariate spherically elliptically symmetric distributions circle trigonometric moments available measure variation discussed aspects maximum likelihood estimation considered likelihood used fit family distributions example set data extension family rotationally symmetric distributions sphere briefly made calibration commonly used survey sampling include auxiliary information estimation stage population parameter calibrating observation weights population means totals set auxiliary variables implies building weights applied auxiliaries give exactly population mean total implicitly calibration techniques rely linear relation survey variable auxiliary variables auxiliary information available units population complex modeling handled means model calibration auxiliary variables used obtain fitted values survey variable units population estimation weights sought satisfy calibration constraints fitted values population mean rather auxiliary variables one work extend model calibration considering general superpopulation models nonparametric methods obtain fitted values calibrate precisely adopt neural network learning local polynomial smoothing estimate functional relationship survey variable auxiliary variables suitable regularity conditions proposed estimators design consistent moments asymptotic distribution derived consistent estimator variance distribution proposed performance proposed estimators finite size samples investigated means simulation application assessment ecological conditions streams mid atlantic highlands united states carried structural equation models sems discussed extensively psychometrics quantitative behavioral sciences many statisticians researchers areas application relatively unfamiliar implementation sem describe basic methods examples environmental epidemiology connections recent work latent variable models multivariate outcomes measurement error methods discuss advantages disadvantages sems compared traditional regressions give detailed example two models fit data well yet one physiologically implausible underscores critical role matter knowledge successful implementation sems brief discussion open research areas included continued debate regarding exact relation cholesterol levels increased respiratory disease mortality one goals reveal relationship subcomponents cholesterol pulmonary function consider subcomponents total cholesterol namely density lipoprotein cholesterol density lipoprotein cholesterol investigate relationship cholesterol levels pulmonary function longitudinal answer questions propose new methodology hierarchical reciprocal graphical models consider identification estimation models propose maximum likelihood estimation generalized em algorithm simulation algorithm corresponding estimates reveals excellent performance proposed procedures application methodology normative aging reveals complicated associations pulmonary function subcomponents total cholesterol recent transplant outcomes donor age cerebrovascular accident cause death cva renal insufficiency serum creatinine mg dl history hypertension identified donor factors associated elevated risk kidney transplant failure great interest know whether remain unmeasured donor factors associated elevated risk graft failure sample deceased donor kidney transplants performed centers addition variation among transplant recipients two random effects unmeasured donor unrecorded center factors data available physician level two random effects crossed two kidneys donor transplanted different centers multivariate frailty models applied analyze data likelihood functions parametric e g piecewise constant baseline hazard semiparametric multivariate frailty models proportional likelihood functions class mixed poisson regression models penalized quasi likelihood method used numerical procedure mixed poisson regression models thus able estimate model crossed random effects structures survival analysis although recipient graft survival rate variation due donor factors explained measured donor characteristics remaining variation among donors graft survival rate still statistically significant suggesting may unmeasured donor factors associated reduced graft survival rate significant variation graft failure rates among transplant centers due unrecorded center factors suggests practice patterns transplant centers identification donor factors may merit investigation estimates technical allocative inefficiencies increase costs therefrom individual firms translog cost system consisting cost function cost share equations call nonlinear random effects system technical allocative inefficiencies random hence term random effects separated random noise terms appearing equation system inefficiency terms appear system highly nonlinear fashion helps separating random errors bayesian inference procedures based markov chain monte carlo mcmc techniques estimate proposed system inferences firm specific technical inefficiency input specific firm specific allocative inefficiencies developed mcmc techniques apply new methods sample u commercial banks focus input allocation problem based assumption banks minimize cost empirical cost top bottom banks increased least due technical inefficiency contrast banks found efficient allocating inputs costs increased average due input misallocation increase costs due technical allocative inefficiencies top bottom banks least translated dollar figures result indicates elimination technical allocative inefficiencies would save top bottom banks million none banks sample exceeded efficient scale size although operating near optimum scale unitary returns scale proactive management web server farms requires accurate prediction workload exemplary measure workload amount service requests per unit time time series workload exhibits short term random fluctuations prominent periodic daily patterns evolve randomly one period another hierarchical framework multiple time scales proposed model time series framework leads adaptive procedure long term days short term minutes predictions simultaneous confidence bands accommodate serial correlation heavy tailedness heteroscedasticity nonstationarity data dna microarrays insight genetic changes characterize different stages disease process accurate identification changes significant therapeutic diagnostic implications statistical analysis multistage multigroup data challenging anova based extensions two sample z tests popular method detecting differentially expressed genes two groups work well multigroup settings false detection rates variability ordinary least squares estimators regression mean induced correlated parameter estimates develop bayesian resealed spike slab hierarchical model specifically designed multigroup gene detection problem data preprocessing steps introduced deal unique features microarray data enhance selection performance theoretically spike slab models naturally encourage sparse solutions process called selective shrinkage translates oracle like gene selection risk performance compared ordinary least squares estimates methodology illustrated large microarray repository samples different clinical stages metastatic colon cancer functional analysis selected genes spike slab models identify biological signals minimizing biologically implausible false detections normalization microarray data essential removing experimental biases revealing meaningful biological motivated problem normalizing microarray data semilinear slide model slim proposed aggregate information arrays slim generalized account across array information resulting even dynamic semiparametric regression model model used normalize microarray data even replication within array semiparametric model number interesting features parametric component nonparametric component primary interest consistently estimated former parametric rate latter nonparametric rate whereas nuisance parameters cannot consistently estimated interesting extension partial consistent phenomena theoretical interest asymptotic normality parametric component rate convergence nonparametric component established augmented simulation illustrated application cdna microarray analysis neuroblastoma cells response macrophage migration inhibitory factor basic question analyzing cdna microarray data normalization purpose remove systematic bias observed expression values establishing normalization curve across whole dynamic range proper normalization procedure ensures normalized intensity ratios meaningful measures relative expression levels propose two way semilinear model tw slm normalization analysis microarray data method usual assumptions underlying existing methods example assume percentage differentially expressed genes small symmetry expression levels regulated regulated genes required lowess normalization method tw slm naturally incorporates uncertainty due normalization significance analysis microarrays semiparametric approach based polynomial splines tw slm estimate normalization curves normalized expression values theoretical properties proposed estimator tw slm including finite sample distributional properties estimated gene effects rate convergence estimated normalization curves number genes large conduct simulation evaluate tw slm method illustrate proposed method published microarray dataset proposes new forecasting method makes information large panel time series like earlier methods method based dynamic factor model argue method improves standard principal component predictor fully exploits dynamic covariance structure panel weights variables according estimated signal noise ratio asymptotic optimal forecast estimator finite samples forecast outperforms standard principal components predictor time series data often measurement error usually result needing estimate variable interest although often reasonable assume measurement error additive e estimator conditionally unbiased missing true value measurement error variances often vary result changes population process time changes sampling effort address estimation parameters linear autoregressive models presence additive uncorrelated measurement errors allowing heteroscedasticity measurement error variances establish asymptotic properties naive estimators ignore measurement error propose estimator based correcting yule walker estimating equations examine pseudo likelihood method based normality assumptions computed kalman filter techniques proposed including two require information measurement error variances compare various estimators theoretically via simulations estimator based corrected estimating equations easy obtain readily accommodates robust unequal measurement error variances asymptotic calculations finite sample simulations often relatively efficient consider semiparametric estimation long memory parameter stationary process presence additive nonparametric mean function semiparametric whittle type estimator applied tapered differenced series mean function necessarily polynomial finite order amount differencing completely remove mean establish central limit theorem estimator memory parameter assuming slowly increasing number frequencies trimmed estimator objective function simulations tapering trimming applied either separately together essential good performance estimator practice simulation compare proposed estimator long memory parameter direct estimator obtained raw data without differencing tapering question feasible inference regression function proposed estimator long memory parameter potentially far less biased direct estimator consequently proposed estimator may lead accurate inference regression function nonparametric methodology longitudinal data analysis becoming increasingly popular analysis multivariate longitudinal data data several time courses recorded considerably less attention despite importance practical data analysis particular need measures estimates capture dependency components vector valued longitudinal data propose analyze simple effective nonparametric method quantify covariation components multivariate longitudinal observations viewed realizations random process includes notion correlation derivatives time shifted versions concept dynamical correlation based scalar product obtained pairs standardized smoothed curves proposed method used observation times irregular matching responses within dimensional data one may construct dynamical correlation matrix serves starting point standard multivariate analysis techniques principal components iliustrate methods via simulations well data five acute phase blood proteins measured longitudinally hemodialysis patients statistical analysis longitudinal data discussed many authors number methods proposed research focused situations observation times independent carry information response variable rely conditional inference procedures given observation times considers different situation independence assumption may hold observation times may carry information response variable inference estimating equation approaches proposed large sample final sample properties proposed methods established methodology applied bladder cancer motivated investigation additive models backfitting algorithms popular multivariate nonparametric fitting techniques inferences models well developed due partially complexity backfitting estimators tools available answer frequently asked questions whether specific additive component significant admits certain parametric form attempt address issues extend generalized likelihood ratio glr tests additive models backfitting estimator null models newly proposed glr follow asymptotically rescaled chi squared distributions scaling constants degrees freedom independent nuisance parameters wilks phenomenon continues hold variety smoothing techniques relaxed models unspecified error distributions glr tests asymptotically optimal terms rates convergence nonparametric testing addition testing parametric additive model propose bias corrected method improve performance glr bias corrected test share wilks type property simulations conducted wilks phenomenon power proposed tests real example used illustrate performance testing approach wilcoxon rank sum test widely used test equality two populations makes fewer distributional assumptions parametric procedures test wilcoxon rank sum test used data independent data clustered tests based generalized estimating equations gees generalize test proposed develop rank sum test used data clustered application rank sum test develop nonparametric test association genetic marker quantitative trait locus give rank sum test equivalence three populations generalizes kruskal wallis test situations clustered data unlike previous rank tests clustered data proposal valid members cluster belong different groups correlation cluster members differs across groups new test proposed testing whether two random vectors independent gieser randles well taskinen kankainen oja introduced discussed multivariate extensions quadrant test blomqvist serves sequel work presents new multivariate extensions kendall tau spearman rho two different approaches discussed first interdirection proportions used estimate cosines angles centered observation vectors differences observation vectors second covariances affine equivariant multivariate signs ranks used test arising two approaches appear asymptotically equivalent vector elliptically symmetric spatial sign versions easy compute data common dimensions practical robust alternatives normal theory methods asymptotic theory developed approximate finite sample null distributions well calculate limiting pitman efficiencies small sample null permutation distributions described simple simulation used compare proposed tests classical wilks test theory illustrated example motivated questions arising field statistical genetics consider problem testing main nested interaction effects unbalanced factorial designs based concept composite linear rank new notion weighted rank proposed asymptotic normality weighted linear rank established mild conditions consistent estimators developed corresponding limiting covariance structure unified framework weighted rank construct test main nested interaction effects unbalanced factorial designs established proposed tests applicable unbalanced designs arbitrary cell replicates greater one per cell limiting distributions null hypotheses pitman alternatives derived monte carlo simulations conducted confirm validity power proposed tests genetic datasets simulated backcross analyzed application proposed tests quantitative trait loci mapping propose class penalized nonparametric maximum likelihood estimators npmles species richness problem penalty term likelihood likelihood estimators lack extreme instability problem estimators constructed conditional likelihood simpler full likelihood full likelihood npmle solution given norris pollock found great accuracy appropriate penalty term conditional likelihood element class estimators simple fast algorithm penalized npmle developed used greatly speed computation unconditional npmle used profile mixture likelihoods based goal attaining stability retaining sensitivity propose adaptive quadratic penalty function systematic simulation wide range scenarios establishes success method relative competitors discuss application gene number estimation expressed sequence tag est data genomics consider frequentist inference parametric component separately nuisance parameter eta semiparametric models based sampling posterior profile likelihood procedure gives first order correct approximation maximum likelihood estimator consistent estimation efficient fisher information without computing derivatives complicated numerical approximations exact bayesian interpretation established certain data dependent prior sampler useful particular nuisance parameter estimable root n rate neither bootstrap validity general automatic variance estimation theoretically justified even nuisance parameter root n consistent bootstrap known valid proposed markov chain monte carlo procedure yield computational savings maximization likelihood required theory verified three examples methods perform well simulations practical utility illustrated two data analyses standard errors parameter estimates widely used empirical work bootstrap often convenient means estimating standard errors conditions bootstrap standard error estimates theoretically justified much attention establishes conditions consistency moving blocks bootstrap estimators variance least squares estimator linear dynamic models dependent data discuss several applications result particular bootstrap standard error estimates bootstrapping studentized simulation inference based bootstrap standard error estimates may considerably accurate small samples inference based closed form asymptotic estimates consider class generalized skew normal distributions useful selection modeling robustness analysis derive class semiparametric estimators location scale parameters central part model estimators consistent asymptotically normal present semiparametric efficiency bound derive locally efficient estimator achieves bound model skewing function correctly specified estimators propose consistent asymptotically normal even model skewing function misspecified compute loss efficiency cases conduct simulation illustrative example method applicable generalized skew elliptical distributions cats clustering transformation smoothing technique nonparametrically estimating clustering large number curves motivating example genetic microarray experiment method general method includes transformation smoothing multiple curves multiple nonparametric testing screening flat curves clustering curves similar shape nonparametrically inferring clustering estimation error rate new class transformed hazard rate models considered contains multiplicative hazards model additive hazards model special cases sieve maximum likelihood estimators derived model parameters estimators regression coefficients consistent asymptotically normal variance achieving semiparametric efficiency bound simulation conducted examine small sample properties proposed estimates real dataset used illustrate approach dimensional contingency tables tend sparse standard goodness fit x cannot used without pooling categories improvement arbitrary pooling goodness fit large n contingency tables propose classes quadratic form based residuals margins multivariate moments order r classes test asymptotically chi squared distributed null marginal residuals useful diagnosing lack fit parametric models r small r proposed better small sample properties asymptotically powerful x useful multivariate binary models related test class limited information estimators based dimensional margins estimators efficiency one commonly used latent trait model binary data customary modeling continuous point referenced data assumes gaussian process often taken stationary models fitted within bayesian framework unknown parameters process assumed random random gaussian process propose novel spatial dirichlet process mixture model produce random spatial process neither gaussian stationary first develop spatial dirichlet process model spatial data discuss properties familiar limitations associated direct dirichlet process models introduce mixing convolving process pure error process examine properties models created dirichlet process mixing bayesian framework implement posterior inference gibbs sampling spatial prediction raises interesting questions handled illustrate approach simulated data well dataset involving precipitation measurements languedoc roussillon region southern france mahalanobis distances long history given sample size n general location scatter estimators m n sigma n define generalized radii r n root x m n sigma n x m n wish trim observations based estimators m n sigma n natural first remove remote ones e largest r n mind define process maps trimming proportion alpha generalized radius observation removed level trimming analyze asymptotic behavior process elliptically contoured distributions limit law depends elliptical family considered sigma n serves estimate underlying scale factor determinant carry monte carlo simulations finite sample sizes outline application assessing fit fixed elliptical family case proportion outlying observations discarded considers new mixture time homogeneous finite markov chains mixing rate movement develops em algorithm maximum likelihood estimation parameters mixture continuous discrete time versions mixture defined estimation considered separately developed methods illustrated application modeling bond ratings migration class mixture models proposed framework modeling population heterogeneity respect rate movement proposed mixture subsumes mover stayer model widely used applications growth mixture modeling become prominent tool studying heterogeneity developmental trajectories within population develop graphical diagnostics detect misspecification growth mixture models regarding number growth classes growth trajectory means covariance structures model misspecification propose different type empirical bayes residual quantify departure procedure begins imputing multiple independent sets growth classes sample called pseudoclass draws form diagnostic plots examine averaged empirical distributions residuals class proposals draw property single set pseudoclass adjusted residuals asymptotically normal known mean co variance underlying model correct methods justified simulation involving two classes linear growth curves differ covariance structures applied longitudinal data randomized field trial tests whether children trajectories aggressive behavior could modified elementary middle school diagnostics lead solution involving mixture three growth classes comparing diagnostics obtained multiple pseudoclasses multiple imputations computational advantage former obtain criterion determining minimum number pseudoclass draws last witnessed development sampling frameworks permit construction markov chains simultaneously traverse parameter model space substantial methodological made period present survey current state art evaluate recent field discuss future research perspectives context drive develop sampling mechanisms degrees efficiency automation medical researchers interested temporal multivariate measurements complex diseases recently begun developing health state models divide space patient characteristics medically distinct clusters current state art health services research uses k means clustering form health states first order markov chain describe transitions states fitting procedure ignores information temporally adjacent observations prevents uncertainty parameter estimation cluster assignments incorporated analysis natural way address issues combine clustering longitudinal analyses hidden markov model fit hidden markov models longitudinal data bayesian methods account uncertainty parameters conditional underlying correctness model potential lack time homogeneity markov chain accounted embedding transition probabilities hierarchical model bayesian shrinkage across time illustrate approach developing hidden markov health state model comparing effectiveness clozapine haloperidol two antipsychotic medications schizophrenia clozapine outperforms haloperidol identify types patients clozapine advantage greatest weakest discuss advantages disadvantages hidden markov models comparison current methodology controlling infectious diseases requires information rates individuals contact propose novel approach modeling contact rates via continuous contact surface realistic flexible representation contact rates currently used methods approach allows modeling sources heterogeneity due age individual effects gender models fitted serologic survey data maximum likelihood involves solving integral equation linking contact surface infection hazards method illustrated two datasets mumps rubella epstein barr virus herpes simplex virus type infection advantages shortcomings method particularly identifiability contact surface discussed demographic analysis data births deaths migration together coverage measurement surveys capture recapture methods established u census counts flawed certain subpopulations previous work census data african americans age proposed hierarchical bayesian model assembled census follow survey demographic data providing principled solution problem negative estimated counts subpopulations smoothing highly variable estimates across subpopulations providing estimates precision incorporate uncertainty demographic analysis estimates extends effort refining hierarchical model design expanding set models considered considering presence bias census follow survey counts obtaining bayes factors model selection applying methods entire u census comparisons u census included well develop model uses repeated observations biological community estimate number composition species community estimators community level attributes constructed model based estimators occurrence individual species incorporate imperfect detection individuals data north american breeding bird survey analyzed illustrate variety ecologically quantities easily constructed estimated model based estimators species occurrence particular compute site specific estimates species richness honor classical notions species area relationships suggest extensions model estimate maps occurrence individual species compute inferences related temporal spatial dynamics biological communities phenotypic characterization rare disease genes poses significant statistical challenge need clear clinical management patients carrying disease gene depends crucially accurate characterization genetically predisposed disease including likelihood occurrence among mutation carriers natural history response treatment propose formal yet practical method controlling bias due ignoring ascertainment defined sampling mechanism quantifying association genotype disease data risk families approach statistically efficient conditioning variables used sampling likelihood adjusted factor function sampling weights strata defined variables requires variables sampling probabilities strata define either known estimated latter requires second population based dataset example derive ascertainment corrected estimates penetrance breast cancer susceptibility genes brca brca bayesian analysis incorporates modified segregation model prior data penetrance derived markov chain monte carlo methods used inference family dimension reduction methods inverse regression ir family developed minimizing quadratic objective function optimal member family inverse regression estimator ire proposed along inference methods computational algorithm ire least three desirable properties estimated basis central dimension reduction subspace asymptotically efficient test statistic dimension asymptotic chi squared distribution chi squared test conditional independence response independent selected subset predictors given remaining predictors current methods like sliced inverse regression belong suboptimal class ir family comparisons methods reported simulation approach developed allows relatively straightforward derivation asymptotic null distribution test statistic dimension used sliced average variance estimation proposes new method analysis partially linear model whose nonlinear component completely unknown target analysis identification set regressors enter nonlinear way model function complete estimation model including slope coefficients linear component link function nonlinear component procedure allows selection significant regression variables develop test linear partially linear alternative generally test nonlinear component m dimensional m approach proposed fully adaptive unknown model structure applies mild conditions model assumption dimensionality nonlinear component relatively small theoretical indicate procedure prescribed level identification error estimates linear component accuracy order n numerical good performance method even small moderate sample sizes survey asymptotic properties regression l p estimators general classes error distributions found asymptotic distributions l estimators depend crucially p shape error distribution near origin number features arise result among small p may yield accelerated convergence rates l p estimators certain classes error distributions b p l p regression circumstances undertaken locally maximizing rather minimizing sum pth powers absolute deviations c consistent estimation sampling distributions l p estimators achieved m n bootstrap general numerical examples provided illustrate theoretical computational algorithm suggested local maximization may sometimes required l p procedure many biomedical observational attempt relate continuous outcome environmental exposure covariates outcome easier cheaper measure relative exposure interest outcome may observed every member finite population whereas exposure measurements may obtained relatively small subsample population rather selecting simple random subsample individuals exposure measurement investigators may attempt enhance efficiency allowing selection probabilities depend observed outcomes refer sampling schemes outcome dependent sampling ods standard estimation methods ignore ods design yield biased inconsistent parameter estimates furthermore generally desirable estimators incorporate available data analyses restricted complete information inefficient end extend estimated likelihood method originally developed discrete outcome measurement error problems accurate exposure measurements made simple random validation sample allow continuous outcomes ods designs derive asymptotic properties proposed estimator simulated data asymptotic closely approximate finite sample properties samples moderate size simulated data compare performance proposed estimator existing methods applicable ods problem semiparametric proportional odds model random effects correlated right censored failure time data establish maximum likelihood estimators parameters model consistent asymptotically gaussian furthermore limiting variances achieve semiparametric efficiency bounds consistently estimated simulation asymptotic approximations accurate practical sample sizes efficiency gains proposed estimators cai cheng wei substantial real example provided illustrate proposed methods truncated survival data arise failure time observed falls within specific truncating set analysis methods rely key assumption quasi independence factorization joint density failure truncation times product proportional individual densities observable region unlike independence failure time censoring time quasi independence tested tests quasi independence available one sided truncation truncation depends measured covariate complex truncation schemes tests quasi independence based multivariate conditional kendall tau proposed doubly truncated data bivariate left truncated data forms truncated survival data arise initiating terminating event times interval censored asymptotic properties null derived tests illustrated several real datasets evaluated via simulation late stage clinical development propose combining phase trials via two stage adaptive design first stage short term safety efficacy examined doses lack efficacy doses cause safety concerns eliminated evaluation trial continues second stage doses eliminated second stage required sample size adjusted maintain power patients including enrolled first stage evaluated clinical endpoint requiring longer follow second stage trend adaptively chosen based estimated dose response curve clinical endpoint first stage patients end trial pairwise first stage adaptive trend second stage clinical endpoint combined establish dose response identify lowest effective dose notable feature proposed approach adaptation rule governing dose selection sample size calculation derivation test second stage need specified advance maintain validity trial phase combination design effective achieving robust statistical power efficient number patients time needed substantially reduced common binary response model propose direct method nonparametric estimation effective dose level estimator obtained composition nonparametric estimate quantile response curve classical density estimate new method yields simple reliable monotone estimate effective dose level curve alpha edalpha appealing users conventional smoothing methods kernel estimators local polynomials series estimators smoothing splines moreover computationally efficient require numerical inversion monotonized estimate quantile dose response curve asymptotic normality new estimate compare available alternative estimate based monotonized nonparametric estimate dose response curve calculation inverse function means simulation estimate effects exposure possibly harmful agent often compare exposed varied doses matched controls zero dose doses measured error one may wish fallible doses estimate linear relationship unobserved true dose observed response one willing assume dose errors exposed symmetrically distributed dose errors pure errors say systematic underreporting exposure presence zero dose controls needed obtain exact distribution free confidence intervals tests consistent point estimates method simpler matched pairs matched sets two matched illustrated two one kind matched pairs first example method uses wilcoxon signed rank test basis inference several zero dose controls matched exposed familiar null distribution signed rank statistic longer applicable dependence within matched sets appropriate exact distribution large sample approximation developed develop procedure analyzing multivariate nonstationary time series slex library smooth localized complex exponentials collection bases basis consisting waveforms orthogonal time localized versions fourier complex exponentials slex framework build family multivariate models explicitly characterize time varying spectral coherence properties every model spectral representation terms unique slex basis selecting model first decompose multivariate time series nonstationary components uncorrelated nonredundant spectral information best slex model selected penalized log energy criterion derive kullback leibler distance model slex principal components multivariate time series model selection criterion takes account pairwise cross correlation simultaneously multivariate time series proposed slex analysis gives easy interpret automatic time dependent generalization classical fourier analysis stationary time series moreover slex method uses computationally efficient algorithms hence able systematic framework extracting spectral features massive dataset illustrate slex analysis application multichannel brain wave dataset recorded epileptic seizure consider tests lack fit arma models nonindependent innovations framework standard box pierce ljung box portmanteau tests perform poorly specifically usual text book formulas asymptotic distributions based strong assumptions applied without careful consideration derive asymptotic covariance matrix sigma rho m vector autocorrelations residuals arma models weak assumptions noise asymptotic distribution portmanteau follows consistent estimator sigma rho m modification portmanteau tests proposed allows us construct valid asymptotic significance limits residual autocorrelations asymptotically valid goodness fit tests underlying noise process assumed noncorrelated rather independent martingale difference set monte carlo experiments application standard poor returns illustrate practical relevance theoretical theory developed bootstrapping unit root tests autoregressive ar context concerned mainly large sample behavior methods proposed assumption null true exist relative performance power behavior bootstrap methods alternative properties different ar bootstrap schemes unit root including new proposal based unrestricted residuals bootstrap procedures based differencing observed series suffer power problems compared bootstrap procedures based unrestricted residuals whereas finite order ar processes differencing leads loss power infinite order autoregressions differencing makes application sieve ar bootstrap schemes inappropriate alternative true superiority new bootstrap proposal numerical examples illustrate theoretical present novel method modeling stationary time series approach construct model specified marginal family build dependence structure around resulting time series linear simple autocorrelation structure construct models parallel existing structures namely state space models autoregressive conditional heteroscedasticity arch models generalized arch models bayesian techniques estimate resulting models models perform well compared competing methods applications considered count models volatility models interested modeling relationship scalar y functional predictor x introduce highly flexible approach called functional adaptive model estimation fame extends generalized linear models glms generalized additive models gams projection pursuit regression ppr handle functional predictors fame approach model standard exponential family response distributions assumed glm gam maintaining flexibility ppr example standard linear logistic regression functional predictors well far complicated models easily applied approach functional principal components decomposition predictor functions aid visualization relationship x y fame procedure extended deal multiple functional standard finite dimensional predictors possibly missing data illustrate fame approach simulated data well prediction arthritis based bone shape end discussion relationships standard regression approaches extensions functional data fame propose nonparametric method perform functional principal components analysis case sparse longitudinal data method aims irregularly spaced longitudinal data number repeated measurements available per small contrast classical functional data analysis requires large number regularly spaced measurements per assume repeated measurements located randomly random number repetitions determined underlying smooth random specific trajectory plus measurement errors basic elements approach parsimonious estimation covariance structure mean function trajectories estimation variance measurement errors eigenfunction basis estimated data functional principal components score estimates obtained conditioning step conditional estimation method conceptually simple straightforward implement key step derivation asymptotic consistency distribution mild conditions tools functional analysis functional data analysis sparse longitudinal data enables prediction individual smooth trajectories even one measurements available asymptotic pointwise simultaneous confidence bands obtained predicted individual trajectories based asymptotic distributions simultaneous bands assumption finite number components model selection techniques akaike information criterion used choose model dimension corresponding number eigenfunctions model methods illustrated simulation longitudinal cd data sample aids patients time course gene expression data yeast cell cycle considers bayesian analysis matched case control problems one covariates partially missing within likelihood context standard approach problem posit fully parametric model among controls partially missing covariate function covariates model variables making strata sometimes strata effects ignored stage approach differs bayesian far importantly manner treats strata effects assume dirichlet process prior normal base measure stratum effects estimate parameters bayesian framework three matched case controt examples simulation considered illustrate methods computing scheme last decade technological generated explosion data substantially smaller sample size relative number covariates p n common goal analysis data involves uncovering group structure observations identifying discriminating variables propose methodology addressing problems simultaneously given set variables formulate clustering problem terms multivariate normal mixture model unknown number components reversible jump markov chain monte carlo technique define sampler moves different dimensional spaces handle problem selecting predictors among prohibitively vast number variable subsets introducing binary exclusion inclusion latent vector gets updated via stochastic search techniques specify conjugate priors exploit conjugacy integrating parameters describe strategies posterior inference explore performance methodology simulated real datasets proposes semiparametric bayesian approach inference unknown isotonic regression function f x characterizing relationship continuous predictor x count response variable y adjusting covariates z dirichlet process mixture poisson distributions used avoid parametric assumptions conditional distribution y given x z avoid parametric assumptions f x novel prior formulation proposed enforces nondecreasing constraint assigns positive prior probability null association carefully tailored hyperprior distributions allow borrowing information across different regions x estimating f x assessing hypotheses local increases function due conjugacy properties posterior computation straightforward markov chain monte carlo algorithm methods illustrated data epidemiologic sleep problems obesity local robustness estimators tests conditional location scale parameters strictly stationary time series model first derive optimal bounded influence estimators settings conditionally gaussian reference model based obtain optimal bounded influence versions classical likelihood based tests parametric hypotheses propose feasible efficient algorithm computation robust estimators uses analytical laplace approximations estimate auxiliary recentering vectors ensuring fisher consistency robust estimation strongly reduces computation time avoiding simulation multidimensional integrals task typically must addressed robust estimation nonlinear models time series monte carlo simulations ar arch process robust procedures maintain efficiency ideal model conditions time perform satisfactorily several forms departure conditional normality contrast classical pseudo maximum likelihood inference procedures found highly inefficient local model misspecifications patterns confirmed application robust testing autoregressive conditional heteroscedasticity focus detection possible outliers based widely used boxplot procedures outliers set data defined subset observations appear inconsistent remaining observations identify outliers constructing boxplot fence lf upper fence uf either satisfying requirement given sample outlier free probability one sample data would fall outside region lf uf equal prescribed small value alpha b taken tolerance limits derived outlier free random sample within specified large proportion sampled population would asserted fall given large probability gamma exact expressions routinely used evaluate constants needed construction boxplot outlier region samples taken family location scale distributions obtained procedures commonly constructed boxplot general inappropriate detecting outliers normal especially exponential samples recommend graphical boxplot constructed based knowledge underlying distribution dataset controling risk labeling regular observations outliers many problems geostatistics response variable interest strongly related underlying geology spatial location situations often little correlation responses found different rock strata underlying covariance structure sharp changes boundaries rock types conventional stationary nonstationary spatial methods inappropriate typically assume covariance points smooth function distance propose generic method analysis spatial data sharp changes underlying covariance structure method works automatically decomposing spatial domain disjoint regions within process assumed stationary data assumed independent across regions uncertainty number disjoint regions shapes model within regions dealt fully bayesian fashion illustrate approach previously unpublished dataset relating soil permeability schneider buda oil field wood county texas phase doppler interferometry pdi nonintrusive technique frequently used obtain information spray characteristics understanding spray characteristics critical importance many areas science including liquid fuel spray combustion spray coatings fire suppression pesticides pdi measures size velocity individual droplets spray due design instrument recordings pdi contain gaps called dead times presence recurring dead times greatly complicates estimation diffusion rate droplets modeling spray process homogeneous poisson process construct consistent asymptotic normal estimators diffusion rate poisson intensity various conditions simulation produced good agreement estimators presence dead time maximum likelihood estimates obtained without dead time experimental data illustrate estimation method elicitation key task subjectivist bayesians although skeptics hold elicitation cannot perhaps done practice brings statisticians closer clients matter expert colleagues reviews state art reflecting experience statisticians informed fruits long line psychological research people represent uncertain information cognitively respond questions information discussion elicitation process first issue address means elicitation successful criteria used answer successful elicitation faithfully represents opinion person elicited necessarily true objectivistic sense cannot judged way see elicitation simply part process statistical modeling indeed hierarchical model point likelihood ends prior begins ambiguous thus kinds judgment inform statistical modeling general inform elicitation prior distributions psychological suggests people prone certain heuristics biases respond situations involving uncertainty result ways asking questions uncertain quantities preferable others appear reliable data lacking exactly well various methods work unclear asking elicitation method person believes consequently one reduced indirect means assessing elicitation methods tool chest methods growing historically first methods involved choosing hyperparameters conjugate prior families time families posterior distributions could computed modern computational methods markov chain monte carlo freed elicitation constraint result parametric nonparametric methods available dimensional problems dimensional problems probably best thought lacking another hierarchical level effect reducing yet unelicited parameter space special considerations apply elicitation group opinions informal methods delphi encourage participants discuss issue hope reaching consensus formal methods weighted averages logarithmic opinion pools mathematical characteristics uncomfortable question group opinion even means necessarily opinion participant broadly speaking nineteenth century bayesian twentieth century frequentist least point view scientific practitioners twenty first century scientists bringing statisticians much bigger problems solve often comprising millions data points thousands parameters statistical philosophy dominate practice guess backed recent examples combination bayesian frequentist ideas needed deal increasingly intense scientific environment challenging period statisticians applied theoretical opens opportunity new golden age rivaling fisher neyman giants early follows text th asa presidential address delivered awards ceremony toronto august take simple time series approach modeling forecasting daily average temperature u cities inquire systematically weather derivatives market answer perhaps supris ingly whether may useful vantage point participants ingly yes time series modeling reveals conditional mean dynamics crucially strong conditional variance dynamics daily average temperature reveals sharp differences distribution temperature distribution temperature surprises argue holds promise producing long horizon predictive densities crucial pricing weather derivatives additional inquiry time series weather forecasting methods likely useful weather derivatives contexts association american railroads wished determine effect maintenance practice known grinding occurrence rail fatigue defects subsequent total traffic usage track must replaced designed experiment practical analysis historical data canadian northern railroad presented analysis certain covariate data available specifically amount grinding physical characteristics rail covariate data available model number defects function traffic usage developed based modulated poisson point process model incorporates effect available covariates mixture dirichlet processes set scale parameters individual rail sections allows assessment overall effect unavailable covariates model used determine optimal replacement period whole rail track analysis grinding reduces expected number defects increases optimal replacement interval numerical indices commonly used tools aid wildfire management hazard assessment although indices widespread assessment indices respective regions application rare evaluate effectiveness burning index bi predicting wildfire occurrences los angeles county california space time point process models models based additive decomposition conditional intensity separate terms used describe spatial seasonal variability well contributions bi fit models wildfire bi data combination nonparametric kernel smoothing methods parametric maximum likelihood addition akaike information criterion aic compare competing models new multidimensional residual methods based approximate random thinning resealing detect departures models ascertain precise contribution bi predicting wildfire occurrence although bi appears positive impact wildfire prediction contribution relatively small taking account natural seasonal spatial variation particular bi appear take account increased activity overpredict early months year call center service network agents telephone based services customers seek services delayed tele queues summarizes analysis unique record call center operations data comprise complete operational history small banking call center call call full year taking perspective queueing theory decompose service process three fundamental components arrivals customer patience service durations component involves different basic mathematical structures requires different style statistical analysis key empirical sketched along descriptions varied techniques required several statistical techniques developed analysis basic components one techniques test point process poisson process another involves estimation mean function nonparametric regression lognormal errors new graphical technique introduced nonparametric hazard rate estimation censored data models developed implemented forecasting poisson arrival rates surveys characteristics deduced statistical analyses form building blocks theoretically interesting practically useful mathematical models call center operations clinical management individuals found harbor mutation known disease susceptibility gene depends accurate assessment mutation specific disease risk missense mutations mms mutations lead single amino acid change protein coded gene poses particularly challenging problem possible predict structural functional changes protein product given amino acid substitution functional assays often available disease association must inferred data individuals mutation inference complicated small sample sizes sampling mechanisms bias toward individuals familial risk disease propose bayesian hierarchical model classify disease association mms given pedigree data collected risk setting model structure allows simultaneous characterization multiple mms uses group pedigrees identified probands tested positive known disease associated mutations group test negative pedigrees obtained clinic calibrate classification control potential ascertainment bias apply model mms breast ovarian susceptibility genes brca brca data collected duke university medical center durham north carolina surveillance epidemiology end seer program national cancer institute authoritative source cancer incidence united states seer program consortium population based cancer registries different areas country registry charged collecting data cancers occur within geographic area disease registry delay time disease cancer first diagnosed time reported registry seer program allowed reporting delays months releasing data public nevertheless additional cases discovered month delay cases added subsequent releases data errors discovered corrected subsequent releases reporting delays corrections typically lead underestimation cancer incidence rates recent diagnosis making difficult monitor trends models account reporting delays corrections predicting eventual cancer counts diagnosis year preliminary counts previous models type studied especially applied aids registries offer several additions existing models first explicitly model reporting corrections second model delay distribution general models combining aspects previous nonparametric like models e models separate parameter delay time parametric models third allow random reporting year effects model practical issues model selection data classified discussed particularly definition reporting correction may change depending subpopulations defined example seer melanoma data studied detail often applied research confidence intervals cis constructed reported parameters selected viewing data selected intervals fail assumed coverage probability generalizing false discover rate fdr approach multiple testing selected multiple cis suggest false coverage statement rate fcr measure interval coverage following selection general procedure introduced offering fcr control level q selection rule procedure constructs marginal ci selected parameter instead confidence level q used marginally q divided number parameters considered multiplied number selected fdr controlling testing procedure benjamini hochberg selecting parameters newly suggested procedure offers cis dual testing procedure optimal independent case positive regression dependency condition benjamini yekutieli fcr controlled one sided tests cis well modification two sided testing general dependency given equivalence cis testing procedure benjamini hochberg offers directional fdr control conjectured consider problem testing k hypotheses simultaneously discuss finite large sample theory stepdown methods control familywise error rate fwe improve bonferroni method holm stepdown method westfall young made effective resampling construct stepdown methods implicitly estimate dependence structure test methods depend assumption known subset pivotality goal construct general stepdown methods require assumption accomplish take close look makes stepdown procedures work key component monotonicity requirement critical values imposing monotonicity estimated critical values assumption model rather assumption method construct stepdown tests applied stagewise fashion k tests need computed moreover stage intersection test controls usual probability type error calculated allows us draw enormous resampling general means test construction addition possible carry method set resamples subsamples intersection tests describe sequential importance sampling sis procedure analyzing two way zero one contingency tables fixed marginal sums essential feature new method samples columns table progressively according certain special distributions method produces monte carlo samples remarkably close uniform distribution enabling one approximate closely null distributions various test tables method compares favorably existing monte carlo based algorithms sometimes orders magnitude efficient particular compared markov chain monte carlo mcmc based approaches importance sampling method efficient terms absolute running time frees one pondering mixing issue easy accurate estimate total number tables fixed marginal sums far difficult mcmc method achieve maximum likelihood ml inference class homogeneous linear predictor hlp models contingency tables described hlp models constrain expected table counts m l m x beta link l allowed many one nonlinear function generalized log linear association trend marginal cumulative probit conditional marginal homogeneity models given specific examples ml fit include point estimates goodness fit asymptotic based approximate distributions described compared equivalent hlp models valid wide variety sampling plans including combinations product multinomial poisson sampling practical implication implementation ml fitting theory straightforward attractive alternative weighted least squares estimation hlp models presents new procedure multifold predictive validation time series procedure based called filtered residuals sample prediction errors evaluated way similar sample ones filtered residuals obtained parameters estimated eliminating estimation process estimated innovations points predicted thus instead deletion observations validate predictions classical cross validation procedure based deletion estimated innovations proved filtered residuals uncorrelated terms small order sample innovations property shared sample residuals parameters needed computing filtered residuals obtained estimating model innovational outliers points predicted proposed multifold predictive validation asymptotically equivalent efficient model selection procedure monte carlo evidence performance procedure presented application illustrated example consider marginal generalized semiparametric partially linear models clustered data lin carroll derived semiparametric efficient score function problem multivariate gaussian case unable construct semiparametric efficient estimator actually achieved semiparametric information bound propose estimator generalize work marginal generalized partially linear models investigate asymptotic relative efficiencies estimators ignore within cluster correlation structure either nonparametric curve estimation throughout evaluate finite sample performance estimators simulations illustrate longitudinal cd cell count dataset theoretical numerical indicate properly taking account within correlation among responses substantially improve efficiency nonrandom association different genes termed linkage disequilibrium ld powerful tool resolution mapping quantitative trait loci qtl underlying complex traits ld based mapping approach made efficient coupled interval mapping characterizing genetic distance markers qtl describes general statistical framework simultaneously estimating linkage ld related two stage hierarchical sampling scheme framework constructed within maximum likelihood context expanded fine scale mapping complex traits different population structures reproductive behaviors closed form solution joint estimation quantitative genetic parameters describing qtl effects qtl position residual variances population genetic parameters describing allele frequencies qtl marker ld perform simulation investigate statistical properties joint analysis model interval ld mapping example body weights dogs multifamily outcrossed pedigree illustrates model analysis censored failure time observations standard cox proportional hazards model assumes regression coefficients time invariant often parameters vary time temporal covariate effects failure time great interest following previous work cai sun propose simple estimation procedure cox model time varying coefficients based kernel weighted partial likelihood approach construct pointwise simultaneous confidence intervals regression parameters properly chosen time interval via simple resampling technique derive prediction method future patients survival specific set covariates building estimates time varying coefficients consider mixed case present estimation procedure time independent parameters model furthermore integrated function estimate specific regression coefficient examine adequacy proportional hazards assumption corresponding covariate graphically numerically proposals illustrated extensively well known mayo clinic consider multivariate temporal processes continuously observed within overlapping time windows intended application censored multistate multivariate survival settings point processes continuously observed data differ functional data like longitudinal data discretely observed irregular times existing functional approaches survival processes intensity models require smoothing depend critically choice smoothing parameters similarly discretely observed data functional mean association regression models point processes unspecified time varying coefficients continuous observation scheme exploited coefficients may estimated nonparametrically extending generalized estimating equations continuously observed data estimators automatically converge parametric rate without smoothing unlike discretely observed data uniform consistency weak convergence established empirical process techniques nonparametric estimators yield new tests covariate effects parametric submodeling effects goodness fit testing simulation analysis familial aggregation alcoholism illustrate methodology practical utility propose calculus designing two stage adaptive procedures describe design components specify two stage adaptive design define interrelationships changes one design component effect changes components technique allows us control many aspects two stage adaptive clinical trial including type type ii error rates maximum total sample size conduct anova type understand effects different components design specification performance characteristics design stage components namely sample size type error rate type ii error rate found influential inverse regression methods facilitate dimension reduction analyses dimensional data extracting small number factors linear combinations original predictor variables estimated factors may lend readily interpretation consistent prior information approach solving problem first incorporate prior information via theory data driven constraints model parameters apply proposed method constrained inverse regression cir extract factors satisfy constraints chi squared tests assess significance factor estimated coefficients generalize cir inverse regression methods situations dimension reduction factor interpretation investigate cir small sample performance test data driven constraints present marketing example illustrate discovering meaningful factors influence desirability brand logos consider identification estimation censored nonparametric location scale model first case location function strictly less fixed censoring point values support explanatory variables location function identified anywhere contrast location function greater equal censoring point positive probability location function identified entire support including region location function censoring point latter case propose simple estimation procedure based combining conditional quantile estimators various quantiles new estimator converge optimal nonparametric rate limiting normal distribution small scale simulation indicates proposed estimation procedure performs well finite samples present empirical illustration unemployment insurance duration administrative level data new jersey record linkage exact matching used join together two files contain information individuals lack unique personal identification codes possibility errors linkage causes problems estimating relationships variables two files effect analogous impact measurement error model linear regression relationship variables linked files proposed assuming probabilities pairs records links known unbiased estimator regression coefficients derived methods estimating linkage probabilities mixture models discussed consistent estimator covariance matrix proposed estimator proposed bootstrap estimator used reflect impact uncertainty record linkage model parameters estimators regression parameters simulation compares performance proposed estimator alternatives bayesian model line signature verification involving representation signature curvature developed prior model makes spatial point process specifying knots approximation restricted buffer region close template curvature along independent time warping mechanism way prior shape information signature built analysis observation model based additive white noise superimposed underlying curvature approach implemented markov chain monte carlo applied collection documented instances william shakespeare signature meta analysis increasing trend explicitly acknowledge presence variability random effects models one assumes specific effect one observing estimate latent variable random effects model one assumes specific effects come distribution one estimate parameters distribution well specific effects distribution often modeled parametric family usually family normal distributions advantage normal distribution mean parameter plays role much focus determining whether mean example may easier justify funding determined mean typically normality assumption made sake convenience rather theoretical justification may actually hold present bayesian model distribution specific effects modeled certain class nonparametric priors priors designed concentrate mass around family normal distributions still allow distribution priors involve univariate parameter plays role mean parameter normal model give rise robust inference parameter present markov chain algorithm estimating posterior distributions model give two illustrations model x x random variables distribution functions f f x said stochastically larger x f f statistical inferences stochastic ordering two sample case long rich history consider k saniple case k populations distribution functions f f f k k assume f f f k k nonparametric maximum likelihood estimators f f order restriction known long time asymptotic distributions derived recently complicated forms hard deal making statistical inferences simple estimators k strongly uniformly consistent asymptotic distributions simple forms f cap f cap empirical restricted estimators f asymptotically p vertical bar f cap x f x vertical bar u p vertical bar f cap x f x vertical bar u x u strict inequality cases clearly uniform improvement restricted estimator unrestricted one consider simultaneous confidence bands test homogeneity stochastic ordering k distributions extended case censored observations examples application real life data provided estimator load share parameters equal load share model derived based observing k component parallel systems identical components continuous distribution function f failure rate r equal load share model first k components fails failure rates remaining components change r gamma r gamma r next failure basis observations n independent identical systems semiparametric estimator component baseline cumulative hazard function r log f presented asymptotic limit process established gaussian process effect estimation load share parameters considered derivation limiting process potential applications found diverse areas including materials testing software reliability power plant safety assessment approximate random variable general chi squared type mixtures random variable form alpha chi d beta via matching first three cumulants approximation error bounds density functions chi squared approximation normal approximation established applications nonparametric goodness fit tests including tests based orthogonal series smoothing splines local polynomial smoothers investigated two simulation conducted compare chi squared approximation normal approximation numerically chi squared approximation illustrated real data example polynomial goodness fit tests discusses asymmetric multiplicative interaction effect capture certain types third order dependence patterns often present social networks dyadic datasets effect along standard linear fixed random effects incorporated generalized linear model markov chain monte carlo algorithm provided bayesian estimation inference example analysis international relations data accounting patterns improves model fit predictive performance considers problem estimating dispersion parameter gaussian model intermediate model mean parameter fully known fixed model mean parameter completely unknown one goals understand implications two step process first selecting model among finite number submodels estimating parameter interest model selection sample data estimators classified global two step weighted estimators whereas global type estimators ignore model space structure two step estimators explore structure adaptively related pretest estimators weighted estimators motivated bayesian approach performances compared theoretically simulations risk functions based scale invariant quadratic loss function variance estimation problem efficiency gains arise exploiting submodel structure two step weighted estimators especially number competing submodels advantage may deteriorate lost altogether two step estimators number submodels increases distance decreases furthermore demonstrated weighted estimators arising properly chosen priors outperform two step estimators many competing submodels submodels close whereas two step estimators preferred submodels highly distinguishable implications model averaging model selection issues work considers number properties space time covariance functions relate spatial temporal interactions process first examines smoothness away origin space time covariance function affects example temporal correlations spatial differences models smoother away origin origin separable models kind discontinuity certain correlations one might wish avoid circumstances smoothness away origin covariance function follow corresponding spectral density derivatives finite moments used obtain parametric class spectral densities whose corresponding space time covariance functions infinitely differentiable away origin allows essentially arbitrary possibly different degrees smoothness process space time second work considers models asymmetric space time covariance site x time site y time different covariance site x time site y time general approach described generating asymmetric models symmetric models taking derivatives implications markov assumption time space time covariance functions gaussian processes examined explicit characterization continuous covariance functions given several new models described work applied wind data ireland causal effects defined comparisons potential outcomes different treatments common set units observed values potential outcomes revealed assignment mechanism probabilistic model treatment unit receives function covariates potential outcomes fisher made tremendous contributions causal inference work design randomized experiments potential outcomes perspective applies complex experiments nonrandomized well noted kempthorne discussion savage fisher lecture fisher never bridged work experimental design work parametric modeling bridge appears nearly automatic appropriate view potential outcomes framework potential outcomes covariates given bayesian distribution complete model specification framework crisply separates scientific inference causal effects decisions based inference distinction evident fisher discussion tests significance versus tests accept reject framework fisher never used potential outcomes framework originally proposed neyman context randomized experiments result provided generally flawed advice concerning analysis covariance adjust posttreatment concomitants randomized trials missing data major issue many applied problems especially biomedical sciences four common approaches inference generalized linear models glms missing covariate data maximum likelihood ml multiple imputation mi fully bayesian fb weighted estimating equations wees considerable interest four methodologies related properties approach advantages disadvantages methodology computational implementation examine data missing random nonignorable missing ml focus techniques em algorithm particular discuss em method weights related procedures discussed ibrahim mi examine techniques developed rubin fb approaches considered ibrahim et al wee focus techniques developed robins et al real dataset detailed simulation compare four methods
